{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as ds\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduler\n",
    "\n",
    "딥러닝 모델을 훈련 시, learning rate를 조정하는 기법. <br>\n",
    "learning rate는 네트워크의 가중치를 얼마나 크게 업데이트할지를 결정하는 하이퍼파라미터로, <br>\n",
    "적절한 학습률을 설정하는 것은 모델이 빠르고 안정적으로 수렴하는 데 매우 중요. <br>\n",
    "\n",
    "learning rate scheduler는 훈련 과정에서 학습률을 동적으로 조정하여 모델 성능을 개선하는 데 기여 <br>\n",
    "- Step Decay: 학습률을 일정한 에포크마다 고정된 비율로 감소\n",
    "- Exponential Decay: 학습률을 지수 함수에 따라 점진적으로 감소\n",
    "- Cyclic Learning Rate: 학습률을 주기적으로 증가와 감소를 반복\n",
    "- Plateau: 검증 손실이 개선되지 않는 경우 학습률을 감소\n",
    "- Combination: 다양한 스케줄러를 조합\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 20pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "configure_optimizers에 scheduler를 추가. <br>\n",
    "return에 scheduler 객체 return 추가. <br>\n",
    "\n",
    "> ```python\n",
    "> def configure_optimizers(self):\n",
    ">     optimizer = optim.Adam(\n",
    ">         self.model.parameter(),\n",
    ">         lr=self.learning_rate,\n",
    ">     )\n",
    ">     scheduler = lr_scheduler.OneCycleLR(\n",
    ">         optimizer,\n",
    ">         max_lr=1e-3,\n",
    ">         total_steps=self.trainer.estimated_stepping_batches,\n",
    ">     )\n",
    ">     return optimizer, scheduler\n",
    "> ````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StepLR\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb2BFko%2FbtqOUlVgZU3%2FOA2OWctZ3wF2Eakhpaf1Y1%2Fimg.png)\n",
    "\n",
    "learning rate를 step_size(epoch)마다 감마로 감쇠. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 16pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "> ```python\n",
    "> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "> ````\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 14pt;\"> Paramter </span>\n",
    "\n",
    "- step_size (int) – 어느 주기로 learning rate를 감소시킬 건지 설정\n",
    "- gamma (float) – 감소될 학습률 (default: 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExponentialLR\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7kmnL%2FbtqOMp5Xu77%2FE8Jk5ydKXYsw71nZcSN831%2Fimg.png)\n",
    "\n",
    "매 epoch마다 이전 learning rate에 gamma만큼 곱하여 감쇄\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 16pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "> ```python\n",
    "> scheduler = ExponentialLR(optimizer, gamma=0.1)\n",
    "> ````\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 14pt;\"> Paramter </span>\n",
    "\n",
    "- gamma (float) – 감소될 학습률 (default: 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CyclicLR\n",
    "\n",
    "|triangular|triangular2|exp_range|\n",
    "|----------|-----------|---------|\n",
    "|![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FVwbCj%2FbtqON8iY9Fp%2FVNODkZFUIbtG3I0wEkEiK1%2Fimg.png)|![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FRb1s0%2FbtqO0glZvNG%2FwHB48CykF7jt4zkzk7ROSk%2Fimg.png)|![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbtIC6z%2FbtqON9vvgpt%2FL5qQdBWHFlzg6GU9Bh2vk1%2Fimg.png)|\n",
    "\n",
    "일정 주기로 설정한 최솟값과 최댓값 사이의 learning rate를 갖도록 함 <br>\n",
    "learning rate를 매 배치 후 변경 <br>\n",
    "- triangular: 선형적으로 증가하고 감소\n",
    "- triangular2: 학습률이 선형적으로 증가하고 감소하지만, 사이클이 진행됨에 따라 최댓값이 줄어듦\n",
    "- exp_range: 지수적으로 증가하고 감소\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 16pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "> ```python\n",
    "> scheduler = CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0)\n",
    "> ````\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 14pt;\"> Paramter </span>\n",
    "\n",
    "- base_lr: 사이클의 최소 learning rate\n",
    "- max_lr: 사이클의 최대 learning rate\n",
    "- step_size_up: learning rate가 base_lr에서 max_lr로 증가하는 데 필요한 단계 수\n",
    "- step_size_down: learning rate가 max_lr에서 다시 base_lr로 감소하는 데 필요한 단계 수.\n",
    "- mode: 사이클의 모양을 지정하는 매개변수\n",
    "    - triangular, triangular2, exp_range 중 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceLROnPlateau\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb25iNZ%2FbtqOKXvn072%2FawCat07o8yb9MHLRHeFUek%2Fimg.png)\n",
    "\n",
    "모델의 성능이 개선되지 않을 때 학습률을 자동으로 감소시켜 학습을 안정화하고 성능을 향상시키기 위한 방법\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 16pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "> ```python\n",
    "> scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=0)\n",
    "> ````\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 14pt;\"> Paramter </span>\n",
    "\n",
    "- mode: 모니터링할 metric의 방향을 설정\n",
    "    - min은 메트릭이 최소일 때, max는 메트릭이 최대일 때 학습률 감소\n",
    "- factor: 학습률 감소 비율로 factor가 0.1이라면 이전 learning rate에 0.1을 곱함\n",
    "- patience: 메트릭이 개선되지 않을 때까지 기다리는 에포크 수\n",
    "- min_lr: 최소 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16972\\2111298443.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 0 2 0 2 0 2 2 1 2 0 2 2 2 1 2 2 1 1 2 0 2 2 0 0 1 0 0 2 2 2 2 1 1 2 2 2\n",
      " 2 0 1 0 1 2 1 2 2 0 2 1 2 2 2 1 2 1 2 2 2 1 2 2 2 0 1 2 2 0 2 2 2 0 2 2 0\n",
      " 0 1 1 2 0 2 2 2 2 2 0 2 2 2 2 2 2 1 0 2 1 1 1 0 2 2 2 2 2 2 1 1 1 0 0 2 0\n",
      " 2 2 2 1 1 2 2 1 1 1 0 2 2 0 2 2 2 1 2 2 2 2 2 2 0 2 2 2 0 2 0 1 2 2 1 2 0\n",
      " 2 2 1 1 2 1 0 0 2 1 2 2 2 2 2 2 2 2 0 2 1 2 1 0 2 1 0 1 2 1 2 0 2 1 2 1 0\n",
      " 2 1 2 1 1 1 1 1 1 2 2 0 2 1 0 1 2 0 2 2 2 0 0 1 2 0 0 1 2 2 0 0 2 1 0 0 2\n",
      " 2 2 2 2 2 2 2 2 2 1 2 0 0 1 2 2 2 0 0 2 0 0 1 0 0 0 1 2 1 2 1 1 0 0 2 2 1\n",
      " 1 0 2 1 2 0 0 0 2 0 0 2 0 1 0 1 1 1 1 1 2 2 2 2 2 2 0 1 2 1 2 2 2 0 0 0 2\n",
      " 2 0 2 2 0 2 2 0 2 2 0 1 2 1 1 0 2 2 0 2 2 2 1 1 1 2 2 2 2 2 1 2 1 2 0 2 1\n",
      " 1 1 2 2 2 2 2 1 1 2 0 1 2 0 0 2 1 0 1 1 2 2 1 0 1 0 2 0 1 0 0 2 0 1 0 2 0\n",
      " 1 2 0 2 2 1 1 2 1 2 2 2 2 2 2 0 0 0 2 2 2 0 0 2 0 0 2 2 2 2 0 0 1 2 2 2 0\n",
      " 0 2 0 1 1 2 0 2 0 2 1 2 1 1 2 2 1 0 0 0 0 2 2 1 0 0 1 2 1 0 1 2 2 0 0 0 2\n",
      " 2 1 2 2 2 2 1 0 0 2 2 1 0 2 1 0 1 0 0 1 0 2 2 0 2 1 2 2 0 1 2 0 2 2 0 1 0\n",
      " 2 2 1 2 2 1 1 2 0 2 2 2 0 1 0 2 0 2 0 2 1 2 1 2 2 0 2 2 0 2 0 2 1 2 2 1 2\n",
      " 1 0 0 2 0 2 2 1 1 2 1 0 1 1 2 2 2 2 0 0 2 2 1 1 2 2 2 0 0 2 2 0 1 2 0 2 0\n",
      " 0 2 2 2 1 1 0 0 0 0 2 1 2 0 1 2 1 2 1 1 0 2 1 1 2 0 2 1 1 2 2 0 0 0 2 2 0\n",
      " 2 1 0 2 1 2 2 2 1 1 2 1 2 0 2 2 0 2 0 2 2 2 2 1 1 2 2 0 2 0 0 2 2 2 2 2 0\n",
      " 1 2 1 0 2 2 2 1 1 0 2 2 2 0 2 1 0 2 2 1 2 2 2 1 2 2 0 2 0 2 2 1 2 1 2 2 0\n",
      " 2 2 2 1 0 2 2 2 2 1 2 2 2 0 1 2 0 0 2 2 1 0 1 1 1 0 2 2 0 0 2 1 2 2 2 0 1\n",
      " 2 2 1 2 2 1 0 0 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n"
     ]
    }
   ],
   "source": [
    "def convert_category_into_integer(df: pd.DataFrame, columns: list):\n",
    "    \"\"\"\n",
    "    주어진 DataFrame의 특정 열들을 범주형에서 정수형으로 변환합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): 변환할 데이터프레임\n",
    "    - columns (list): 범주형에서 정수형으로 변환할 열 이름의 리스트\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: 변환된 데이터프레임\n",
    "    - dict: 각 열에 대해 적합한 LabelEncoder 객체를 포함하는 딕셔너리\n",
    "    \"\"\"\n",
    "    label_encoders = {}  # 각 열의 LabelEncoder 객체를 저장할 딕셔너리입니다.\n",
    "    \n",
    "    for column in columns:\n",
    "        # 각 열에 대해 LabelEncoder 객체를 생성합니다.\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # LabelEncoder를 사용하여 해당 열의 범주형 데이터를 정수형으로 변환합니다.\n",
    "        df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
    "        \n",
    "        # 변환된 LabelEncoder 객체를 딕셔너리에 저장합니다.\n",
    "        label_encoders.update({column: label_encoder})\n",
    "    \n",
    "    # 변환된 데이터프레임과 LabelEncoder 객체를 포함하는 딕셔너리를 반환합니다.\n",
    "    return df, label_encoders\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.deck = titanic.deck.astype(str)\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "titanic, _ = convert_category_into_integer(titanic, ('sex', 'embarked', 'class', 'who', 'deck', 'embark_town', 'alive'))\n",
    "\n",
    "titanic = titanic.astype(np.float32)\n",
    "\n",
    "train, temp = train_test_split(titanic, test_size=0.4, random_state=seed)\n",
    "\n",
    "# 임시 데이터를 검증용 데이터와 테스트용 데이터로 분할\n",
    "# 임시 데이터의 절반을 검증용 데이터로, 나머지 절반을 테스트용 데이터로 사용\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, data):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화합니다.\n",
    "        self.data = data  # 데이터프레임을 저장합니다.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 데이터셋의 전체 샘플 수를 반환합니다.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 `idx`에 해당하는 데이터 샘플을 반환합니다.\n",
    "        \n",
    "        # 데이터프레임에서 'survived' 열을 제외한 특성값을 가져와서 NumPy 배열로 변환한 뒤, PyTorch 텐서로 변환합니다.\n",
    "        X = torch.from_numpy(self.data.drop('survived', axis=1).iloc[idx].values).float()\n",
    "        \n",
    "        # 'survived' 열의 값을 텐서로 변환하여 레이블을 생성합니다.\n",
    "        y = torch.tensor(self.data.iloc[idx].survived).long()\n",
    "        \n",
    "        # 입력 데이터와 레이블을 딕셔너리 형태로 반환합니다.\n",
    "        return {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "        }\n",
    "\n",
    "# 배치 크기 (Batch Size): 한 번의 업데이트에서 사용할 데이터 샘플 수\n",
    "batch_size = 128\n",
    "\n",
    "# 에포크 수 (Epochs): 전체 데이터셋을 몇 번 반복하여 학습할 것인지 지정\n",
    "epochs = 20\n",
    "\n",
    "# 학습률 (Learning Rate): 모델 파라미터를 업데이트할 때 사용되는 학습률\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 은닉층의 뉴런 수 (Hidden Dimension): 은닉층의 차원 또는 뉴런 수\n",
    "hidden_dim = 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 시드를 설정하여 실험의 재현성을 보장합니다.\n",
    "seed = 6\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)  # NumPy의 시드를 설정합니다.\n",
    "torch.manual_seed(seed)  # PyTorch의 시드를 설정합니다.\n",
    "\n",
    "# 장치가 CUDA(GPU)일 경우, CUDA의 시드를 설정합니다.\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)  # 현재 장치의 CUDA 시드를 설정합니다.\n",
    "    torch.cuda.manual_seed_all(seed)  # 모든 CUDA 장치의 시드를 설정합니다.\n",
    "    torch.backends.cudnn.deterministic = False  # Deterministic 연산을 비활성화합니다. 비활성화하면 더 빠른 연산이 가능할 수 있습니다.\n",
    "    torch.backends.cudnn.benchmark = True  # 벤치마크를 활성화하여 최적의 성능을 위해 CUDA 커널을 자동으로 튜닝합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7160\\1783900557.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 0 2 0 2 0 2 2 1 2 0 2 2 2 1 2 2 1 1 2 0 2 2 0 0 1 0 0 2 2 2 2 1 1 2 2 2\n",
      " 2 0 1 0 1 2 1 2 2 0 2 1 2 2 2 1 2 1 2 2 2 1 2 2 2 0 1 2 2 0 2 2 2 0 2 2 0\n",
      " 0 1 1 2 0 2 2 2 2 2 0 2 2 2 2 2 2 1 0 2 1 1 1 0 2 2 2 2 2 2 1 1 1 0 0 2 0\n",
      " 2 2 2 1 1 2 2 1 1 1 0 2 2 0 2 2 2 1 2 2 2 2 2 2 0 2 2 2 0 2 0 1 2 2 1 2 0\n",
      " 2 2 1 1 2 1 0 0 2 1 2 2 2 2 2 2 2 2 0 2 1 2 1 0 2 1 0 1 2 1 2 0 2 1 2 1 0\n",
      " 2 1 2 1 1 1 1 1 1 2 2 0 2 1 0 1 2 0 2 2 2 0 0 1 2 0 0 1 2 2 0 0 2 1 0 0 2\n",
      " 2 2 2 2 2 2 2 2 2 1 2 0 0 1 2 2 2 0 0 2 0 0 1 0 0 0 1 2 1 2 1 1 0 0 2 2 1\n",
      " 1 0 2 1 2 0 0 0 2 0 0 2 0 1 0 1 1 1 1 1 2 2 2 2 2 2 0 1 2 1 2 2 2 0 0 0 2\n",
      " 2 0 2 2 0 2 2 0 2 2 0 1 2 1 1 0 2 2 0 2 2 2 1 1 1 2 2 2 2 2 1 2 1 2 0 2 1\n",
      " 1 1 2 2 2 2 2 1 1 2 0 1 2 0 0 2 1 0 1 1 2 2 1 0 1 0 2 0 1 0 0 2 0 1 0 2 0\n",
      " 1 2 0 2 2 1 1 2 1 2 2 2 2 2 2 0 0 0 2 2 2 0 0 2 0 0 2 2 2 2 0 0 1 2 2 2 0\n",
      " 0 2 0 1 1 2 0 2 0 2 1 2 1 1 2 2 1 0 0 0 0 2 2 1 0 0 1 2 1 0 1 2 2 0 0 0 2\n",
      " 2 1 2 2 2 2 1 0 0 2 2 1 0 2 1 0 1 0 0 1 0 2 2 0 2 1 2 2 0 1 2 0 2 2 0 1 0\n",
      " 2 2 1 2 2 1 1 2 0 2 2 2 0 1 0 2 0 2 0 2 1 2 1 2 2 0 2 2 0 2 0 2 1 2 2 1 2\n",
      " 1 0 0 2 0 2 2 1 1 2 1 0 1 1 2 2 2 2 0 0 2 2 1 1 2 2 2 0 0 2 2 0 1 2 0 2 0\n",
      " 0 2 2 2 1 1 0 0 0 0 2 1 2 0 1 2 1 2 1 1 0 2 1 1 2 0 2 1 1 2 2 0 0 0 2 2 0\n",
      " 2 1 0 2 1 2 2 2 1 1 2 1 2 0 2 2 0 2 0 2 2 2 2 1 1 2 2 0 2 0 0 2 2 2 2 2 0\n",
      " 1 2 1 0 2 2 2 1 1 0 2 2 2 0 2 1 0 2 2 1 2 2 2 1 2 2 0 2 0 2 2 1 2 1 2 2 0\n",
      " 2 2 2 1 0 2 2 2 2 1 2 2 2 0 1 2 0 0 2 2 1 0 1 1 1 0 2 2 0 0 2 1 2 2 2 0 1\n",
      " 2 2 1 2 2 1 0 0 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 시드를 설정하여 실험의 재현성을 보장합니다.\n",
    "seed = 6\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)  # NumPy의 시드를 설정합니다.\n",
    "torch.manual_seed(seed)  # PyTorch의 시드를 설정합니다.\n",
    "\n",
    "# 장치가 CUDA(GPU)일 경우, CUDA의 시드를 설정합니다.\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)  # 현재 장치의 CUDA 시드를 설정합니다.\n",
    "    torch.cuda.manual_seed_all(seed)  # 모든 CUDA 장치의 시드를 설정합니다.\n",
    "    torch.backends.cudnn.deterministic = False  # Deterministic 연산을 비활성화합니다. 비활성화하면 더 빠른 연산이 가능할 수 있습니다.\n",
    "    torch.backends.cudnn.benchmark = True  # 벤치마크를 활성화하여 최적의 성능을 위해 CUDA 커널을 자동으로 튜닝합니다.\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.deck = titanic.deck.astype(str)\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "titanic, _ = convert_category_into_integer(titanic, ('sex', 'embarked', 'class', 'who', 'deck', 'embark_town', 'alive'))\n",
    "\n",
    "titanic = titanic.astype(np.float32)\n",
    "\n",
    "train, temp = train_test_split(titanic, test_size=0.4, random_state=seed)\n",
    "\n",
    "# 임시 데이터를 검증용 데이터와 테스트용 데이터로 분할\n",
    "# 임시 데이터의 절반을 검증용 데이터로, 나머지 절반을 테스트용 데이터로 사용\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, data):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화합니다.\n",
    "        self.data = data  # 데이터프레임을 저장합니다.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 데이터셋의 전체 샘플 수를 반환합니다.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 `idx`에 해당하는 데이터 샘플을 반환합니다.\n",
    "        \n",
    "        # 데이터프레임에서 'survived' 열을 제외한 특성값을 가져와서 NumPy 배열로 변환한 뒤, PyTorch 텐서로 변환합니다.\n",
    "        X = torch.from_numpy(self.data.drop('survived', axis=1).iloc[idx].values).float()\n",
    "        \n",
    "        # 'survived' 열의 값을 텐서로 변환하여 레이블을 생성합니다.\n",
    "        y = torch.tensor(self.data.iloc[idx].survived).long()\n",
    "        \n",
    "        # 입력 데이터와 레이블을 딕셔너리 형태로 반환합니다.\n",
    "        return {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "        }\n",
    "\n",
    "# 배치 크기 (Batch Size): 한 번의 업데이트에서 사용할 데이터 샘플 수\n",
    "batch_size = 128\n",
    "\n",
    "# 에포크 수 (Epochs): 전체 데이터셋을 몇 번 반복하여 학습할 것인지 지정\n",
    "epochs = 20\n",
    "\n",
    "# 학습률 (Learning Rate): 모델 파라미터를 업데이트할 때 사용되는 학습률\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 은닉층의 뉴런 수 (Hidden Dimension): 은닉층의 차원 또는 뉴런 수\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7160\\1783900557.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 0 2 0 2 0 2 2 1 2 0 2 2 2 1 2 2 1 1 2 0 2 2 0 0 1 0 0 2 2 2 2 1 1 2 2 2\n",
      " 2 0 1 0 1 2 1 2 2 0 2 1 2 2 2 1 2 1 2 2 2 1 2 2 2 0 1 2 2 0 2 2 2 0 2 2 0\n",
      " 0 1 1 2 0 2 2 2 2 2 0 2 2 2 2 2 2 1 0 2 1 1 1 0 2 2 2 2 2 2 1 1 1 0 0 2 0\n",
      " 2 2 2 1 1 2 2 1 1 1 0 2 2 0 2 2 2 1 2 2 2 2 2 2 0 2 2 2 0 2 0 1 2 2 1 2 0\n",
      " 2 2 1 1 2 1 0 0 2 1 2 2 2 2 2 2 2 2 0 2 1 2 1 0 2 1 0 1 2 1 2 0 2 1 2 1 0\n",
      " 2 1 2 1 1 1 1 1 1 2 2 0 2 1 0 1 2 0 2 2 2 0 0 1 2 0 0 1 2 2 0 0 2 1 0 0 2\n",
      " 2 2 2 2 2 2 2 2 2 1 2 0 0 1 2 2 2 0 0 2 0 0 1 0 0 0 1 2 1 2 1 1 0 0 2 2 1\n",
      " 1 0 2 1 2 0 0 0 2 0 0 2 0 1 0 1 1 1 1 1 2 2 2 2 2 2 0 1 2 1 2 2 2 0 0 0 2\n",
      " 2 0 2 2 0 2 2 0 2 2 0 1 2 1 1 0 2 2 0 2 2 2 1 1 1 2 2 2 2 2 1 2 1 2 0 2 1\n",
      " 1 1 2 2 2 2 2 1 1 2 0 1 2 0 0 2 1 0 1 1 2 2 1 0 1 0 2 0 1 0 0 2 0 1 0 2 0\n",
      " 1 2 0 2 2 1 1 2 1 2 2 2 2 2 2 0 0 0 2 2 2 0 0 2 0 0 2 2 2 2 0 0 1 2 2 2 0\n",
      " 0 2 0 1 1 2 0 2 0 2 1 2 1 1 2 2 1 0 0 0 0 2 2 1 0 0 1 2 1 0 1 2 2 0 0 0 2\n",
      " 2 1 2 2 2 2 1 0 0 2 2 1 0 2 1 0 1 0 0 1 0 2 2 0 2 1 2 2 0 1 2 0 2 2 0 1 0\n",
      " 2 2 1 2 2 1 1 2 0 2 2 2 0 1 0 2 0 2 0 2 1 2 1 2 2 0 2 2 0 2 0 2 1 2 2 1 2\n",
      " 1 0 0 2 0 2 2 1 1 2 1 0 1 1 2 2 2 2 0 0 2 2 1 1 2 2 2 0 0 2 2 0 1 2 0 2 0\n",
      " 0 2 2 2 1 1 0 0 0 0 2 1 2 0 1 2 1 2 1 1 0 2 1 1 2 0 2 1 1 2 2 0 0 0 2 2 0\n",
      " 2 1 0 2 1 2 2 2 1 1 2 1 2 0 2 2 0 2 0 2 2 2 2 1 1 2 2 0 2 0 0 2 2 2 2 2 0\n",
      " 1 2 1 0 2 2 2 1 1 0 2 2 2 0 2 1 0 2 2 1 2 2 2 1 2 2 0 2 0 2 2 1 2 1 2 2 0\n",
      " 2 2 2 1 0 2 2 2 2 1 2 2 2 0 1 2 0 0 2 2 1 0 1 1 1 0 2 2 0 0 2 1 2 2 2 0 1\n",
      " 2 2 1 2 2 1 0 0 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7160\\1783900557.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 0 2 0 2 0 2 2 1 2 0 2 2 2 1 2 2 1 1 2 0 2 2 0 0 1 0 0 2 2 2 2 1 1 2 2 2\n",
      " 2 0 1 0 1 2 1 2 2 0 2 1 2 2 2 1 2 1 2 2 2 1 2 2 2 0 1 2 2 0 2 2 2 0 2 2 0\n",
      " 0 1 1 2 0 2 2 2 2 2 0 2 2 2 2 2 2 1 0 2 1 1 1 0 2 2 2 2 2 2 1 1 1 0 0 2 0\n",
      " 2 2 2 1 1 2 2 1 1 1 0 2 2 0 2 2 2 1 2 2 2 2 2 2 0 2 2 2 0 2 0 1 2 2 1 2 0\n",
      " 2 2 1 1 2 1 0 0 2 1 2 2 2 2 2 2 2 2 0 2 1 2 1 0 2 1 0 1 2 1 2 0 2 1 2 1 0\n",
      " 2 1 2 1 1 1 1 1 1 2 2 0 2 1 0 1 2 0 2 2 2 0 0 1 2 0 0 1 2 2 0 0 2 1 0 0 2\n",
      " 2 2 2 2 2 2 2 2 2 1 2 0 0 1 2 2 2 0 0 2 0 0 1 0 0 0 1 2 1 2 1 1 0 0 2 2 1\n",
      " 1 0 2 1 2 0 0 0 2 0 0 2 0 1 0 1 1 1 1 1 2 2 2 2 2 2 0 1 2 1 2 2 2 0 0 0 2\n",
      " 2 0 2 2 0 2 2 0 2 2 0 1 2 1 1 0 2 2 0 2 2 2 1 1 1 2 2 2 2 2 1 2 1 2 0 2 1\n",
      " 1 1 2 2 2 2 2 1 1 2 0 1 2 0 0 2 1 0 1 1 2 2 1 0 1 0 2 0 1 0 0 2 0 1 0 2 0\n",
      " 1 2 0 2 2 1 1 2 1 2 2 2 2 2 2 0 0 0 2 2 2 0 0 2 0 0 2 2 2 2 0 0 1 2 2 2 0\n",
      " 0 2 0 1 1 2 0 2 0 2 1 2 1 1 2 2 1 0 0 0 0 2 2 1 0 0 1 2 1 0 1 2 2 0 0 0 2\n",
      " 2 1 2 2 2 2 1 0 0 2 2 1 0 2 1 0 1 0 0 1 0 2 2 0 2 1 2 2 0 1 2 0 2 2 0 1 0\n",
      " 2 2 1 2 2 1 1 2 0 2 2 2 0 1 0 2 0 2 0 2 1 2 1 2 2 0 2 2 0 2 0 2 1 2 2 1 2\n",
      " 1 0 0 2 0 2 2 1 1 2 1 0 1 1 2 2 2 2 0 0 2 2 1 1 2 2 2 0 0 2 2 0 1 2 0 2 0\n",
      " 0 2 2 2 1 1 0 0 0 0 2 1 2 0 1 2 1 2 1 1 0 2 1 1 2 0 2 1 1 2 2 0 0 0 2 2 0\n",
      " 2 1 0 2 1 2 2 2 1 1 2 1 2 0 2 2 0 2 0 2 2 2 2 1 1 2 2 0 2 0 0 2 2 2 2 2 0\n",
      " 1 2 1 0 2 2 2 1 1 0 2 2 2 0 2 1 0 2 2 1 2 2 2 1 2 2 0 2 0 2 2 1 2 1 2 2 0\n",
      " 2 2 2 1 0 2 2 2 2 1 2 2 2 0 1 2 0 0 2 2 1 0 1 1 1 0 2 2 0 0 2 1 2 2 2 0 1\n",
      " 2 2 1 2 2 1 0 0 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type  | Params | Mode \n",
      "----------------------------------------\n",
      "0 | model | Model | 1.1 K  | train\n",
      "----------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 941:  43%|████▎     | 3/7 [00:00<00:00, 47.53it/s, v_num=3, acc/val_acc=1.000, loss/val_loss=0.313, acc/train_acc=1.000, loss/train_loss=0.313] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[57], line 48\u001b[0m, in \u001b[0;36mTitanicDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 'survived' 열의 값을 텐서로 변환하여 레이블을 생성합니다.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurvived\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 입력 데이터와 레이블을 딕셔너리 형태로 반환합니다.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 238\u001b[0m\n\u001b[0;32m    225\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    226\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mepochs,  \u001b[38;5;66;03m# 학습할 최대 에포크 수\u001b[39;00m\n\u001b[0;32m    227\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     ),\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# PyTorch Lightning Trainer를 사용하여 모델 학습을 시작\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# 'titanic_module'은 학습할 모델과 학습 및 검증 루프를 정의한 LightningModule 인스턴스\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# 'titanic_data_module'은 데이터셋을 로드하고 데이터로더를 제공하는 LightningDataModule 인스턴스\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitanic_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 학습할 모델 인스턴스\u001b[39;49;00m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitanic_data_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 데이터셋과 데이터로더를 제공하는 데이터 모듈 인스턴스\u001b[39;49;00m\n\u001b[0;32m    241\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 시드를 설정하여 실험의 재현성을 보장합니다.\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)  # NumPy의 시드를 설정합니다.\n",
    "torch.manual_seed(seed)  # PyTorch의 시드를 설정합니다.\n",
    "\n",
    "# 장치가 CUDA(GPU)일 경우, CUDA의 시드를 설정합니다.\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)  # 현재 장치의 CUDA 시드를 설정합니다.\n",
    "    torch.cuda.manual_seed_all(seed)  # 모든 CUDA 장치의 시드를 설정합니다.\n",
    "    torch.backends.cudnn.deterministic = False  # Deterministic 연산을 비활성화합니다. 비활성화하면 더 빠른 연산이 가능할 수 있습니다.\n",
    "    torch.backends.cudnn.benchmark = True  # 벤치마크를 활성화하여 최적의 성능을 위해 CUDA 커널을 자동으로 튜닝합니다.\n",
    "\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.deck = titanic.deck.astype(str)\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "titanic, _ = convert_category_into_integer(titanic, ('sex', 'embarked', 'class', 'who', 'deck', 'embark_town', 'alive'))\n",
    "\n",
    "titanic = titanic.astype(np.float32)\n",
    "\n",
    "train, temp = train_test_split(titanic, test_size=0.4, random_state=seed)\n",
    "\n",
    "# 임시 데이터를 검증용 데이터와 테스트용 데이터로 분할\n",
    "# 임시 데이터의 절반을 검증용 데이터로, 나머지 절반을 테스트용 데이터로 사용\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, data):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화합니다.\n",
    "        self.data = data  # 데이터프레임을 저장합니다.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 데이터셋의 전체 샘플 수를 반환합니다.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 `idx`에 해당하는 데이터 샘플을 반환합니다.\n",
    "        \n",
    "        # 데이터프레임에서 'survived' 열을 제외한 특성값을 가져와서 NumPy 배열로 변환한 뒤, PyTorch 텐서로 변환합니다.\n",
    "        X = torch.from_numpy(self.data.drop('survived', axis=1).iloc[idx].values).float()\n",
    "        \n",
    "        # 'survived' 열의 값을 텐서로 변환하여 레이블을 생성합니다.\n",
    "        y = torch.tensor(self.data.iloc[idx].survived).long()\n",
    "        \n",
    "        # 입력 데이터와 레이블을 딕셔너리 형태로 반환합니다.\n",
    "        return {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "        }\n",
    "\n",
    "# 배치 크기 (Batch Size): 한 번의 업데이트에서 사용할 데이터 샘플 수\n",
    "batch_size = 64\n",
    "\n",
    "# 에포크 수 (Epochs): 전체 데이터셋을 몇 번 반복하여 학습할 것인지 지정\n",
    "epochs = 1000\n",
    "\n",
    "# 학습률 (Learning Rate): 모델 파라미터를 업데이트할 때 사용되는 학습률\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 은닉층의 뉴런 수 (Hidden Dimension): 은닉층의 차원 또는 뉴런 수\n",
    "hidden_dim = 64\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터를 TitanicDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "train_dataset = TitanicDataset(train)\n",
    "\n",
    "# 검증 데이터를 TitanicDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "valid_dataset = TitanicDataset(valid)\n",
    "\n",
    "# 테스트 데이터를 TitanicDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "test_dataset = TitanicDataset(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):  # nn.Module을 상속받아 새로운 모델 클래스를 정의합니다.\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화합니다.\n",
    "        self.input_dim = input_dim  # 입력 차원 크기를 저장합니다.\n",
    "        self.hidden_dim = hidden_dim  # 숨겨진 층의 차원 크기를 저장합니다.\n",
    "        self.output_dim = output_dim  # 출력 차원 크기를 저장합니다.\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)  # 입력 차원에서 숨겨진 차원으로의 선형 변환을 정의합니다.\n",
    "        self.relu = nn.ReLU()  # ReLU 활성화 함수를 정의합니다.\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)  # 숨겨진 차원에서 출력 차원으로의 선형 변환을 정의합니다.\n",
    "    \n",
    "    def forward(self, x):  # 순전파 메서드\n",
    "        x = self.linear(x)  # 입력 데이터에 대해 선형 변환을 적용합니다.\n",
    "        x = self.relu(x)  # ReLU 활성화 함수를 적용하여 비선형성을 추가합니다.\n",
    "        x = self.output(x)  # 두 번째 선형 변환을 적용하여 최종 출력을 계산합니다.\n",
    "\n",
    "        return x  # 최종 출력을 반환합니다.\n",
    "    \n",
    "# len(titanic.columns) - 1: 입력 특성의 수 (타이타닉 데이터셋에서 'survived' 열을 제외한 나머지 열)\n",
    "# hidden_dim: 은닉층의 뉴런 수 (변수로 설정된 값)\n",
    "# 2: 출력 클래스의 수 (이진 분류 문제이므로 두 개 클래스)\n",
    "model = Model(len(titanic.columns) - 1, hidden_dim, 2)\n",
    "\n",
    "class TitanicDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare(self, train_dataset, valid_dataset, test_dataset):     \n",
    "        self.train_dataset, self.valid_dataset, self.test_dataset = train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":      \n",
    "            self.train_data = self.train_dataset\n",
    "            self.valid_data = self.valid_dataset\n",
    "\n",
    "        if stage == \"test\":     \n",
    "            self.test_data = self.test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.valid_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    \n",
    "titanic_data_module = TitanicDataModule(batch_size=batch_size)\n",
    "titanic_data_module.prepare(train_dataset, valid_dataset, test_dataset)\n",
    "\n",
    "class TitanicModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,    # 구축한 모델\n",
    "        learning_rate: float,      # 학습률\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model         # 모델 초기화\n",
    "        self.learning_rate = learning_rate  # 학습률 초기화\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "\n",
    "        X = batch.get('X')  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y')  # 레이블 데이터를 장치로 이동\n",
    "        y = y.squeeze()  # 레이블의 차원을 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        logit = F.softmax(output, dim=-1)  # 소프트맥스 활성화 함수 적용\n",
    "        self.loss = F.cross_entropy(logit, y)  # 손실 계산\n",
    "\n",
    "        predicted_label = logit.argmax(dim=-1)  # 예측된 레이블 계산\n",
    "        self.acc = (predicted_label == y).float().mean()  # 정확도 계산\n",
    "\n",
    "\n",
    "        return self.loss  # 손실 반환\n",
    "    \n",
    "    def on_train_epoch_end(self, *args, **kwargs):\n",
    "        # 학습 에포크가 끝날 때 호출되는 메서드\n",
    "        self.log_dict({'acc/train_acc': self.acc, 'loss/train_loss': self.loss}, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "\n",
    "        X = batch.get('X')  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y')  # 레이블 데이터를 장치로 이동\n",
    "        y = y.squeeze()  # 레이블의 차원을 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        logit = F.softmax(output, dim=-1)  # 소프트맥스 활성화 함수 적용\n",
    "        self.val_loss = F.cross_entropy(logit, y)  # 검증 손실 계산\n",
    "\n",
    "        predicted_label = logit.argmax(dim=-1)  # 예측된 레이블 계산\n",
    "        self.val_acc = (predicted_label == y).float().mean()  # 정확도 계산\n",
    "\n",
    "\n",
    "        return self.val_loss  # 검증 손실 반환\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # 검증 에포크가 끝날 때 호출되는 메서드\n",
    "        self.log_dict({'acc/val_acc': self.val_acc, 'loss/val_loss': self.val_loss}, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # 테스트 단계에서 호출되는 메서드\n",
    "        X = batch.get('X').to(device)  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y').to(device)  # 레이블 데이터를 장치로 이동\n",
    "        y = y.squeeze()  # 레이블의 차원을 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        logit = F.softmax(output, dim=-1)  # 소프트맥스 활성화 함수 적용\n",
    "        predicted_label = logit.argmax(dim=-1)  # 예측된 레이블 계산\n",
    "\n",
    "        return predicted_label  # 예측된 레이블 반환\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 옵티마이저를 설정하는 메서드\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),  # 모델 파라미터를 옵티마이저에 전달\n",
    "            lr=self.learning_rate,    # 학습률 설정\n",
    "        )\n",
    "\n",
    "        return {'optimizer': optimizer}  # 옵티마이저 반환\n",
    "    \n",
    "# TitanicModule 클래스의 인스턴스를 생성합니다.\n",
    "titanic_module = TitanicModule(\n",
    "    model=model,              # 학습할 모델 인스턴스 (예: Model 객체)\n",
    "    learning_rate=learning_rate  # 학습률 (예: 1e-3)\n",
    ")\n",
    "# Trainer 인스턴스 생성\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,  # 학습할 최대 에포크 수\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss/val_loss', mode='min', patience= 5)\n",
    "    ],\n",
    "    logger= TensorBoardLogger(\n",
    "        \"tensorboard\",                                                # root folder\n",
    "        f\"seed= {seed}, batch_size= {batch_size}, learning_rate= {learning_rate}, hidden_dim= {hidden_dim}\",   # folder name\n",
    "    ),\n",
    ")\n",
    "# PyTorch Lightning Trainer를 사용하여 모델 학습을 시작\n",
    "# 'titanic_module'은 학습할 모델과 학습 및 검증 루프를 정의한 LightningModule 인스턴스\n",
    "# 'titanic_data_module'은 데이터셋을 로드하고 데이터로더를 제공하는 LightningDataModule 인스턴스\n",
    "trainer.fit(\n",
    "    model=titanic_module,       # 학습할 모델 인스턴스\n",
    "    datamodule=titanic_data_module,  # 데이터셋과 데이터로더를 제공하는 데이터 모듈 인스턴스\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53789</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53790</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53791</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.66</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53792</th>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53793</th>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53794 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z\n",
       "0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
       "...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n",
       "53789   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50\n",
       "53790   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61\n",
       "53791   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56\n",
       "53792   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74\n",
       "53793   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64\n",
       "\n",
       "[53794 rows x 10 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7160\\1783900557.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 3 1 ... 4 3 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7160\\1783900557.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 1 1 ... 0 4 0]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7160\\1783900557.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[3 2 4 ... 2 3 3]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53789</th>\n",
       "      <td>0.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>60.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53790</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>63.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53791</th>\n",
       "      <td>0.70</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>62.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.66</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53792</th>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53793</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53794 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  cut  color  clarity  depth  table  price     x     y     z\n",
       "0       0.23    2      1        3   61.5   55.0    326  3.95  3.98  2.43\n",
       "1       0.21    3      1        2   59.8   61.0    326  3.89  3.84  2.31\n",
       "2       0.23    1      1        4   56.9   65.0    327  4.05  4.07  2.31\n",
       "3       0.29    3      5        5   62.4   58.0    334  4.20  4.23  2.63\n",
       "4       0.31    1      6        3   63.3   58.0    335  4.34  4.35  2.75\n",
       "...      ...  ...    ...      ...    ...    ...    ...   ...   ...   ...\n",
       "53789   0.72    2      0        2   60.8   57.0   2757  5.75  5.76  3.50\n",
       "53790   0.72    1      0        2   63.1   55.0   2757  5.69  5.75  3.61\n",
       "53791   0.70    4      0        2   62.8   60.0   2757  5.66  5.68  3.56\n",
       "53792   0.86    3      4        3   61.0   58.0   2757  6.15  6.12  3.74\n",
       "53793   0.75    2      0        3   62.2   55.0   2757  5.83  5.87  3.64\n",
       "\n",
       "[53794 rows x 10 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "diamonds = diamonds.drop_duplicates().reset_index(drop=True)\n",
    "diamonds, _ = convert_category_into_integer(diamonds, ('cut', 'color', 'clarity'))\n",
    "\n",
    "diamonds\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "train.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
    "    standard_scaler.fit_transform(train.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] )\n",
    "\n",
    "# 검증 세트의 동일한 열을 훈련 세트에서 계산된 평균과 표준편차를 사용하여 표준화합니다.\n",
    "valid.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
    "    standard_scaler.transform(valid.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] )\n",
    "\n",
    "# 테스트 세트의 동일한 열을 훈련 세트에서 계산된 평균과 표준편차를 사용하여 표준화합니다.\n",
    "test.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
    "    standard_scaler.transform(test.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16972\\2111298443.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 3 1 ... 4 3 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16972\\2111298443.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 1 1 ... 0 4 0]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16972\\2111298443.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[3 2 4 ... 2 3 3]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16972\\1255949588.py:29: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.77360262  2.34732793 -0.82616882 ... -0.65069783 -0.62741851\n",
      " -0.17409759]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16972\\1255949588.py:33: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.82016125  0.61439545  1.97961482 ... -0.83042418 -0.11201941\n",
      " -0.7618378 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  valid.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16972\\1255949588.py:37: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.73480375  0.18235133  1.97185505 ...  2.94658264 -0.0364242\n",
      "  1.48899693]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  test.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:377: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "\n",
      "  | Name  | Type  | Params | Mode \n",
      "----------------------------------------\n",
      "0 | model | Model | 353    | train\n",
      "----------------------------------------\n",
      "353       Trainable params\n",
      "0         Non-trainable params\n",
      "353       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (32) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 32/32 [00:09<00:00,  3.38it/s, v_num=0, loss/val_loss=0.127, learning_rate=0.100, loss/train_loss=0.264] \n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 시드를 설정하여 실험의 재현성을 보장합니다.\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)  # NumPy의 시드를 설정합니다.\n",
    "torch.manual_seed(seed)  # PyTorch의 시드를 설정합니다.\n",
    "\n",
    "# 장치가 CUDA(GPU)일 경우, CUDA의 시드를 설정합니다.\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)  # 현재 장치의 CUDA 시드를 설정합니다.\n",
    "    torch.cuda.manual_seed_all(seed)  # 모든 CUDA 장치의 시드를 설정합니다.\n",
    "    torch.backends.cudnn.deterministic = False  # Deterministic 연산을 비활성화합니다. 비활성화하면 더 빠른 연산이 가능할 수 있습니다.\n",
    "    torch.backends.cudnn.benchmark = True  # 벤치마크를 활성화하여 최적의 성능을 위해 CUDA 커널을 자동으로 튜닝합니다.\n",
    "\n",
    "\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "diamonds = diamonds.drop_duplicates().reset_index(drop=True)\n",
    "diamonds, _ = convert_category_into_integer(diamonds, ('cut', 'color', 'clarity'))\n",
    "\n",
    "train, temp = train_test_split(diamonds, test_size=0.4, random_state=seed)\n",
    "\n",
    "# 임시 데이터를 검증용 데이터와 테스트용 데이터로 분할\n",
    "# 임시 데이터의 절반을 검증용 데이터로, 나머지 절반을 테스트용 데이터로 사용\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "train.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
    "    standard_scaler.fit_transform(train.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] )\n",
    "\n",
    "# 검증 세트의 동일한 열을 훈련 세트에서 계산된 평균과 표준편차를 사용하여 표준화합니다.\n",
    "valid.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
    "    standard_scaler.transform(valid.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] )\n",
    "\n",
    "# 테스트 세트의 동일한 열을 훈련 세트에서 계산된 평균과 표준편차를 사용하여 표준화합니다.\n",
    "test.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
    "    standard_scaler.transform(test.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] )\n",
    "\n",
    "\n",
    "\n",
    "class DiamondsDataset(Dataset):\n",
    "    def __init__(self, data):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화합니다.\n",
    "        self.data = data  # 데이터프레임을 저장합니다.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 데이터셋의 전체 샘플 수를 반환합니다.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 `idx`에 해당하는 데이터 샘플을 반환합니다.\n",
    "        \n",
    "        # 데이터프레임에서 'price' 열을 제외한 특성값을 가져와서 NumPy 배열로 변환한 뒤, PyTorch 텐서로 변환합니다.\n",
    "        X = torch.from_numpy(self.data.iloc[idx].drop('price').values).float()\n",
    "        \n",
    "        # 'price' 열의 값을 텐서로 변환하여 레이블을 생성합니다.\n",
    "        y = torch.Tensor([self.data.iloc[idx].price]).float()\n",
    "        \n",
    "        # 입력 데이터와 레이블을 딕셔너리 형태로 반환합니다.\n",
    "        return {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "        }\n",
    "\n",
    "# 배치 크기 (Batch Size): 한 번의 업데이트에서 사용할 데이터 샘플 수\n",
    "batch_size = 1024  # 데이터 배치의 크기를 설정합니다.\n",
    "epochs = 100  # 학습 에포크 수를 설정합니다.\n",
    "learning_rate = 1e-1  # 학습률을 설정합니다.\n",
    "hidden_dim = 32  # 숨겨진 층의 차원 크기를 설정합니다.\n",
    "dropout_prob = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터를 TitanicDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "train_dataset = DiamondsDataset(train)\n",
    "\n",
    "# 검증 데이터를 TitanicDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "valid_dataset = DiamondsDataset(valid)\n",
    "\n",
    "# 테스트 데이터를 TitanicDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "test_dataset = DiamondsDataset(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):  # nn.Module을 상속받아 새로운 모델 클래스를 정의합니다.\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화합니다.\n",
    "        self.input_dim = input_dim  # 입력 차원 크기를 저장합니다.\n",
    "        self.hidden_dim = hidden_dim  # 숨겨진 층의 차원 크기를 저장합니다.\n",
    "        self.output_dim = output_dim  # 출력 차원 크기를 저장합니다.\n",
    "        self.dropout_prob =dropout_prob\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)  # 입력 차원에서 숨겨진 차원으로의 선형 변환을 정의합니다.\n",
    "        self.relu = nn.ReLU()  # ReLU 활성화 함수를 정의합니다.\n",
    "        self.dropout1 = nn.Dropout(self.dropout_prob)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)  # 숨겨진 차원에서 출력 차원으로의 선형 변환을 정의합니다.\n",
    "    \n",
    "    def forward(self, x):  # 순전파 메서드\n",
    "        x = self.linear(x)  # 입력 데이터에 대해 선형 변환을 적용합니다.\n",
    "        x = self.relu(x)  # ReLU 활성화 함수를 적용하여 비선형성을 추가합니다.\n",
    "        x = self.dropout1(x)\n",
    "        x = self.output(x)  # 두 번째 선형 변환을 적용하여 최종 출력을 계산합니다.\n",
    "\n",
    "        return x  # 최종 출력을 반환합니다.\n",
    "    \n",
    "# len(titanic.columns) - 1: 입력 특성의 수 (타이타닉 데이터셋에서 'survived' 열을 제외한 나머지 열)\n",
    "# hidden_dim: 은닉층의 뉴런 수 (변수로 설정된 값)\n",
    "# 2: 출력 클래스의 수 (이진 분류 문제이므로 두 개 클래스)\n",
    "# Model 클래스를 사용하여 모델 인스턴스를 생성합니다.\n",
    "model = Model(\n",
    "    input_dim=len(diamonds.columns)-1,  # 입력 차원 크기를 설정합니다. diamonds 데이터프레임의 열 개수에서 1을 뺀 값입니다.\n",
    "    hidden_dim=hidden_dim,  # 숨겨진 층의 차원 크기를 설정합니다. 이 값은 미리 정의된 `hidden_dim` 변수로부터 가져옵니다.\n",
    "    output_dim=1,  # 출력 차원 크기를 설정합니다. 'cut' 열의 고유한 값의 개수를 가져옵니다.\n",
    "    dropout_prob =dropout_prob,\n",
    ").to(device)\n",
    "\n",
    "class DiamondsDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare(self, train_dataset, valid_dataset, test_dataset):     \n",
    "        self.train_dataset, self.valid_dataset, self.test_dataset = train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":      \n",
    "            self.train_data = self.train_dataset\n",
    "            self.valid_data = self.valid_dataset\n",
    "\n",
    "        if stage == \"test\":     \n",
    "            self.test_data = self.test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.valid_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    \n",
    "diamonds_data_module = DiamondsDataModule(batch_size=batch_size)\n",
    "diamonds_data_module.prepare(train_dataset, valid_dataset, test_dataset)\n",
    "\n",
    "class DiamondsModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,    # 구축한 모델\n",
    "        learning_rate: float,      # 학습률\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model         # 모델 초기화\n",
    "        self.learning_rate = learning_rate  # 학습률 초기화\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "\n",
    "        X = batch.get('X')  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y')  # 레이블 데이터를 장치로 이동\n",
    "         # 레이블의 차원을 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        #logit = F.softmax(output, dim=-1)  # 소프트맥스 활성화 함수 적용\n",
    "        self.loss = F.mse_loss(output, y)  # 손실 계산\n",
    "\n",
    "        # predicted_label = self.loss.argmax(dim=-1)  # 예측된 레이블 계산\n",
    "        # self.acc = (predicted_label == y).float().mean()  # 정확도 계산\n",
    "\n",
    "\n",
    "        return self.loss  # 손실 반환\n",
    "    \n",
    "    def on_train_epoch_end(self, *args, **kwargs):\n",
    "        # 학습 에포크가 끝날 때 호출되는 메서드\n",
    "        self.log_dict({'loss/train_loss': self.loss}, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "       \n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "\n",
    "        X = batch.get('X')  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y')  # 레이블 데이터를 장치로 이동  # 레이블의 차원을 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        # logit = F.softmax(output, dim=-1)  # 소프트맥스 활성화 함수 적용\n",
    "        self.val_loss = F.mse_loss(output, y)  # 검증 손실 계산\n",
    "\n",
    "        predicted_label = self.val_loss.argmax(dim=-1)  # 예측된 레이블 계산\n",
    "        self.val_acc = (predicted_label == y).float().mean()  # 정확도 계산\n",
    "\n",
    "\n",
    "        return self.val_loss  # 검증 손실 반환\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # 검증 에포크가 끝날 때 호출되는 메서드\n",
    "        self.log_dict({'loss/val_loss': self.val_loss}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log_dict({'learning_rate': self.learning_rate}, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # 테스트 단계에서 호출되는 메서드\n",
    "        X = batch.get('X').to(device)  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y').to(device)  # 레이블 데이터를 장치로 이동\n",
    "        y = y.squeeze()  # 레이블의 차원을 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "    \n",
    "\n",
    "        return output  # 예측된 레이블 반환\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 옵티마이저를 설정하는 메서드\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),  # 모델 파라미터를 옵티마이저에 전달\n",
    "            lr=self.learning_rate,    # 학습률 설정\n",
    "        )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode= 'min',\n",
    "            factor= 0.5,\n",
    "            patience= 5,\n",
    "        )\n",
    "   \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }  # 옵티마이저 반환\n",
    "    \n",
    "# TitanicModule 클래스의 인스턴스를 생성합니다.\n",
    "diamonds_module = DiamondsModule(\n",
    "    model=model,              # 학습할 모델 인스턴스 (예: Model 객체)\n",
    "    learning_rate=learning_rate  # 학습률 (예: 1e-3)\n",
    ")\n",
    "# Trainer 인스턴스 생성\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,  # 학습할 최대 에포크 수\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss/val_loss', mode='min', patience= 5)\n",
    "    ],\n",
    "    logger= TensorBoardLogger(\n",
    "        \"tensorboard\",                                                # root folder\n",
    "        f\"seed= {seed}, batch_size= {batch_size}, learning_rate= {learning_rate}, hidden_dim= {hidden_dim}, dropout_prob={dropout_prob}\",   # folder name\n",
    "    ),\n",
    ")\n",
    "# PyTorch Lightning Trainer를 사용하여 모델 학습을 시작\n",
    "# 'titanic_module'은 학습할 모델과 학습 및 검증 루프를 정의한 LightningModule 인스턴스\n",
    "# 'titanic_data_module'은 데이터셋을 로드하고 데이터로더를 제공하는 LightningDataModule 인스턴스\n",
    "trainer.fit(\n",
    "    model=diamonds_module,       # 학습할 모델 인스턴스\n",
    "    datamodule=diamonds_data_module,  # 데이터셋과 데이터로더를 제공하는 데이터 모듈 인스턴스\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> class Model(nn.Module):\n",
    ">     def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.5):\n",
    ">         super().__init__()\n",
    ">         self.input_dim = input_dim\n",
    ">         self.hidden_dim = hidden_dim\n",
    ">         self.output_dim = output_dim\n",
    "> \n",
    ">         self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    ">         self.relu1 = nn.ReLU()\n",
    ">         self.dropout1 = nn.Dropout(dropout_prob)  # 첫 번째 드롭아웃\n",
    ">         self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    ">         self.relu2 = nn.ReLU()\n",
    ">         self.dropout2 = nn.Dropout(dropout_prob)  # 두 번째 드롭아웃\n",
    ">         self.output = nn.Linear(hidden_dim, output_dim)\n",
    ">     \n",
    ">     def forward(self, x):\n",
    ">         x = self.linear1(x)\n",
    ">         x = self.relu1(x)\n",
    ">         x = self.dropout1(x)  # 첫 번째 드롭아웃 적용\n",
    ">         x = self.linear2(x)\n",
    ">         x = self.relu2(x)\n",
    ">         x = self.dropout2(x)  # 두 번째 드롭아웃 적용\n",
    ">         x = self.output(x)\n",
    "> \n",
    ">         return x\n",
    "> ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
