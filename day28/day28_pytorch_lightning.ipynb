{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as ds\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\334355076.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 3 1 ... 4 3 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  diamonds.loc[:, column] = label_encoder.fit_transform(diamonds[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\334355076.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 1 1 ... 0 4 0]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  diamonds.loc[:, column] = label_encoder.fit_transform(diamonds[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\334355076.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[3 2 4 ... 2 3 3]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  diamonds.loc[:, column] = label_encoder.fit_transform(diamonds[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\334355076.py:37: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.77360262  2.34732793 -0.82616882 ... -0.65069783 -0.62741851\n",
      " -0.17409759]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']] = standard_scaler.fit_transform(train.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\334355076.py:38: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.82016125  0.61439545  1.97961482 ... -0.83042418 -0.11201941\n",
      " -0.7618378 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  valid.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']] = standard_scaler.transform(valid.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\334355076.py:39: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.73480375  0.18235133  1.97185505 ...  2.94658264 -0.0364242\n",
      "  1.48899693]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  test.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']] = standard_scaler.transform(test.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
      "  1%|          | 1/100 [00:12<20:59, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss:  1.2623 | train_acc:  64.29% | valid_loss:  1.2169 | valid_acc:  68.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:25<21:07, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss:  1.2174 | train_acc:  68.73% | valid_loss:  1.2083 | valid_acc:  69.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [01:00<36:37, 22.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss:  1.2112 | train_acc:  69.35% | valid_loss:  1.2067 | valid_acc:  69.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [01:11<29:06, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss:  1.2094 | train_acc:  69.47% | valid_loss:  1.2036 | valid_acc:  70.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [01:22<24:48, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss:  1.2104 | train_acc:  69.33% | valid_loss:  1.2027 | valid_acc:  70.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [01:33<22:11, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss:  1.2091 | train_acc:  69.35% | valid_loss:  1.2075 | valid_acc:  69.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [01:45<20:39, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss:  1.2075 | train_acc:  69.59% | valid_loss:  1.2043 | valid_acc:  70.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [01:56<19:22, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | train_loss:  1.2076 | train_acc:  69.61% | valid_loss:  1.2071 | valid_acc:  69.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [02:07<18:30, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | train_loss:  1.2062 | train_acc:  69.75% | valid_loss:  1.2039 | valid_acc:  70.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [02:19<18:00, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | train_loss:  1.2063 | train_acc:  69.78% | valid_loss:  1.2100 | valid_acc:  69.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [02:31<17:46, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | train_loss:  1.2072 | train_acc:  69.66% | valid_loss:  1.2043 | valid_acc:  69.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [02:43<17:30, 11.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | train_loss:  1.2059 | train_acc:  69.79% | valid_loss:  1.1972 | valid_acc:  70.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [02:54<17:03, 11.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | train_loss:  1.2067 | train_acc:  69.69% | valid_loss:  1.2161 | valid_acc:  68.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [03:05<16:39, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | train_loss:  1.2043 | train_acc:  69.95% | valid_loss:  1.1985 | valid_acc:  70.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [03:17<16:28, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | train_loss:  1.2018 | train_acc:  70.21% | valid_loss:  1.2009 | valid_acc:  70.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [03:29<16:23, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | train_loss:  1.2009 | train_acc:  70.26% | valid_loss:  1.1974 | valid_acc:  70.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [03:41<16:22, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | train_loss:  1.1985 | train_acc:  70.42% | valid_loss:  1.1872 | valid_acc:  71.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [03:53<16:16, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | train_loss:  1.1947 | train_acc:  70.83% | valid_loss:  1.1875 | valid_acc:  71.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [04:05<16:04, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | train_loss:  1.1934 | train_acc:  70.88% | valid_loss:  1.1912 | valid_acc:  71.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [04:17<15:53, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | train_loss:  1.1889 | train_acc:  71.46% | valid_loss:  1.1880 | valid_acc:  71.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [04:29<15:42, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | train_loss:  1.1908 | train_acc:  71.21% | valid_loss:  1.1853 | valid_acc:  71.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [04:48<18:22, 14.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | train_loss:  1.1908 | train_acc:  71.21% | valid_loss:  1.1827 | valid_acc:  72.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [05:49<35:56, 28.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | train_loss:  1.1875 | train_acc:  71.52% | valid_loss:  1.1847 | valid_acc:  71.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [06:56<50:31, 39.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | train_loss:  1.1867 | train_acc:  71.57% | valid_loss:  1.1886 | valid_acc:  71.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [08:01<59:21, 47.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | train_loss:  1.1843 | train_acc:  71.90% | valid_loss:  1.1781 | valid_acc:  72.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [08:21<48:15, 39.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | train_loss:  1.1850 | train_acc:  71.82% | valid_loss:  1.1818 | valid_acc:  72.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [08:33<37:44, 31.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | train_loss:  1.1878 | train_acc:  71.53% | valid_loss:  1.1761 | valid_acc:  72.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [08:45<30:21, 25.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | train_loss:  1.1840 | train_acc:  71.89% | valid_loss:  1.1744 | valid_acc:  72.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [08:57<25:10, 21.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | train_loss:  1.1847 | train_acc:  71.84% | valid_loss:  1.1803 | valid_acc:  72.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [09:09<21:31, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | train_loss:  1.1851 | train_acc:  71.73% | valid_loss:  1.1757 | valid_acc:  72.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [09:20<18:48, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | train_loss:  1.1819 | train_acc:  72.07% | valid_loss:  1.1717 | valid_acc:  73.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [09:32<16:54, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | train_loss:  1.1802 | train_acc:  72.31% | valid_loss:  1.1718 | valid_acc:  73.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [09:43<15:32, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | train_loss:  1.1786 | train_acc:  72.45% | valid_loss:  1.1698 | valid_acc:  73.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [09:55<14:39, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | train_loss:  1.1782 | train_acc:  72.43% | valid_loss:  1.1710 | valid_acc:  73.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [10:51<28:08, 25.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | train_loss:  1.1777 | train_acc:  72.50% | valid_loss:  1.1803 | valid_acc:  72.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [12:02<42:17, 39.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | train_loss:  1.1777 | train_acc:  72.52% | valid_loss:  1.1690 | valid_acc:  73.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [12:49<43:50, 41.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | train_loss:  1.1765 | train_acc:  72.58% | valid_loss:  1.1699 | valid_acc:  73.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [13:00<33:38, 32.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | train_loss:  1.1719 | train_acc:  73.03% | valid_loss:  1.1682 | valid_acc:  73.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [13:12<26:38, 26.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | train_loss:  1.1734 | train_acc:  72.80% | valid_loss:  1.1656 | valid_acc:  73.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [13:23<21:40, 21.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | train_loss:  1.1631 | train_acc:  73.88% | valid_loss:  1.1507 | valid_acc:  75.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [13:34<18:12, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | train_loss:  1.1558 | train_acc:  74.75% | valid_loss:  1.1410 | valid_acc:  76.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [13:45<15:45, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | train_loss:  1.1551 | train_acc:  74.77% | valid_loss:  1.1458 | valid_acc:  75.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [13:57<14:09, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | train_loss:  1.1482 | train_acc:  75.49% | valid_loss:  1.1383 | valid_acc:  76.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [14:08<12:58, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | train_loss:  1.1518 | train_acc:  75.14% | valid_loss:  1.1395 | valid_acc:  76.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [14:19<11:59, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | train_loss:  1.1485 | train_acc:  75.27% | valid_loss:  1.1363 | valid_acc:  76.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [14:31<11:16, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | train_loss:  1.1477 | train_acc:  75.44% | valid_loss:  1.1351 | valid_acc:  77.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [14:42<10:45, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | train_loss:  1.1506 | train_acc:  75.24% | valid_loss:  1.1424 | valid_acc:  76.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [14:53<10:17, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | train_loss:  1.1479 | train_acc:  75.46% | valid_loss:  1.1387 | valid_acc:  76.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [15:05<10:09, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | train_loss:  1.1500 | train_acc:  75.30% | valid_loss:  1.1406 | valid_acc:  76.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [15:17<09:53, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | train_loss:  1.1467 | train_acc:  75.65% | valid_loss:  1.1316 | valid_acc:  77.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [15:28<09:37, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | train_loss:  1.1453 | train_acc:  75.71% | valid_loss:  1.1548 | valid_acc:  74.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [15:40<09:18, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | train_loss:  1.1470 | train_acc:  75.49% | valid_loss:  1.1684 | valid_acc:  73.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [15:51<09:02, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | train_loss:  1.1477 | train_acc:  75.41% | valid_loss:  1.1390 | valid_acc:  76.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [16:03<08:49, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | train_loss:  1.1437 | train_acc:  75.95% | valid_loss:  1.1537 | valid_acc:  74.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [16:14<08:32, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | train_loss:  1.1468 | train_acc:  75.62% | valid_loss:  1.1462 | valid_acc:  75.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [16:21<13:22, 17.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 126\u001b[0m\n\u001b[0;32m    122\u001b[0m total_train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    124\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m    127\u001b[0m     X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    128\u001b[0m     y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[73], line 50\u001b[0m, in \u001b[0;36mDiamondsDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 50\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcut\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# .values 판다스를 numpy로 바꿔줌\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx]\u001b[38;5;241m.\u001b[39mcut])\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;66;03m# tensor는 단순 float을 tensor로 바꿔주는데  []안에 넣어야함\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m : X,\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m : y,\n\u001b[0;32m     56\u001b[0m     }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# seed\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# hyperparameter\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 1e-2\n",
    "hidden_dim = 32\n",
    "\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "\n",
    "label_encoders = {}\n",
    "for column in ('cut', 'color', 'clarity'):\n",
    "    label_encoder = LabelEncoder()\n",
    "    diamonds.loc[:, column] = label_encoder.fit_transform(diamonds[column])\n",
    "\n",
    "    label_encoders.update({column: label_encoder})\n",
    "\n",
    "\n",
    "diamonds = diamonds.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "train, temp = train_test_split(diamonds, test_size=0.4, random_state=seed)\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "train.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']] = standard_scaler.fit_transform(train.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
    "valid.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']] = standard_scaler.transform(valid.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
    "test.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']] = standard_scaler.transform(test.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
    "\n",
    "class DiamondsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.from_numpy(self.data.iloc[idx].drop('cut').values).float() # .values 판다스를 numpy로 바꿔줌\n",
    "        y = torch.tensor([self.data.iloc[idx].cut]).long() # tensor는 단순 float을 tensor로 바꿔주는데  []안에 넣어야함\n",
    "\n",
    "        return {\n",
    "            'X' : X,\n",
    "            'y' : y,\n",
    "        }\n",
    "    \n",
    "\n",
    "train_dataset = DiamondsDataset(train)\n",
    "valid_dataset = DiamondsDataset(valid)\n",
    "test_dataset = DiamondsDataset(test)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Model(\n",
    "    input_dim= len(diamonds.columns)-1,\n",
    "    hidden_dim= hidden_dim,\n",
    "    output_dim= train['cut'].nunique(),\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    ##################### Train ###########################\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for data in train_dataloader:\n",
    "        X = data.get('X').to(device)\n",
    "        y = data.get('y').to(device)\n",
    "        y = y.squeeze()\n",
    "        \n",
    "        optimizer.zero_grad() # 기울기 초기화\n",
    "        output = model(X)\n",
    "        logit = F.softmax(output, dim =-1)\n",
    "        train_loss = criterion(logit, y) \n",
    "        train_loss.backward()\n",
    "        optimizer.step() # 파라미터 업데이트\n",
    "\n",
    "        train_acc = (logit.argmax(dim=-1) == y).float().mean()\n",
    "        total_train_loss += train_loss\n",
    "        total_train_acc += train_acc\n",
    "\n",
    "    mean_train_loss = total_train_loss /len(train_dataloader)\n",
    "    mean_train_acc = total_train_acc /len(train_dataloader)\n",
    "    train_losses.append(mean_train_loss)\n",
    "    train_accs.append(mean_train_acc)\n",
    "    \n",
    "    ##################### Validation #############################\n",
    "\n",
    "    total_valid_loss = 0\n",
    "    total_valid_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # valid는 기울기를 안써서 없애줌줌\n",
    "        for data in valid_dataloader:\n",
    "            X = data.get('X').to(device)\n",
    "            y = data.get('y').to(device)\n",
    "            y = y.squeeze() # [32, 1]에서 1을 날려줌 [32]\n",
    "    \n",
    "            output = model(X)\n",
    "            logit = F.softmax(output, dim= -1)\n",
    "            valid_loss = criterion(logit, y)\n",
    "\n",
    "            valid_acc = (logit.argmax(dim=-1) == y).float().mean()\n",
    "            total_valid_loss += valid_loss\n",
    "            total_valid_acc += valid_acc\n",
    "\n",
    "    mean_valid_loss = total_valid_loss /len(valid_dataloader)\n",
    "    mean_valid_acc = total_valid_acc /len(valid_dataloader)\n",
    "    valid_losses.append(mean_valid_loss)\n",
    "    valid_accs.append(mean_valid_acc)\n",
    "\n",
    "    print(f'Epoch: {epoch} | train_loss: {mean_train_loss: .4f} | train_acc: {mean_train_acc*100: .2f}% | valid_loss: {mean_valid_loss: .4f} | valid_acc: {mean_valid_acc*100: .2f}%'  )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4132712427.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 3 1 ... 4 3 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  diamonds.loc[:, column] = label_encoder.fit_transform(diamonds[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4132712427.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 1 1 ... 0 4 0]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  diamonds.loc[:, column] = label_encoder.fit_transform(diamonds[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4132712427.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[3 2 4 ... 2 3 3]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  diamonds.loc[:, column] = label_encoder.fit_transform(diamonds[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4132712427.py:37: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.63513455 -0.30119299 -0.77875719 ...  1.11206386 -0.72474095\n",
      "  0.4658882 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train.loc[:, ['carat', 'depth', 'table', 'price','x', 'y', 'z']] = standard_scaler.fit_transform(train.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4132712427.py:38: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.35387491 -0.42941847 -0.23026511 ... -0.74493393  2.71993065\n",
      "  0.88994098]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  valid.loc[:, ['carat', 'depth', 'table', 'price','x', 'y', 'z']] = standard_scaler.transform(valid.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4132712427.py:39: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.24124119 -0.51019043 -0.78632956 ... -0.66113303 -0.49479328\n",
      " -0.1951798 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  test.loc[:, ['carat', 'depth', 'table', 'price','x', 'y', 'z']] = standard_scaler.transform(test.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
      "  1%|          | 1/100 [00:10<17:39, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss:  0.0860 | valid_loss:  0.0774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:21<17:19, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss:  0.0745 | valid_loss:  0.0826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:31<17:03, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss:  0.0934 | valid_loss:  0.0834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:42<16:47, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss:  0.0581 | valid_loss:  0.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [01:01<21:23, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss:  0.0521 | valid_loss:  0.1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [01:16<21:59, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss:  0.0516 | valid_loss:  0.1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [01:26<20:00, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss:  0.0517 | valid_loss:  0.1320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [01:36<18:32, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | train_loss:  0.0559 | valid_loss:  0.1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [01:47<17:32, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | train_loss:  0.0608 | valid_loss:  0.1115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [02:13<23:56, 15.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | train_loss:  0.0511 | valid_loss:  0.1336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [02:41<29:04, 19.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | train_loss:  0.0529 | valid_loss:  0.1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [02:52<24:55, 16.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | train_loss:  0.0534 | valid_loss:  0.1462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [03:02<21:46, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | train_loss:  0.0495 | valid_loss:  0.1587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [03:34<28:47, 20.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | train_loss:  0.0542 | valid_loss:  0.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [03:59<30:46, 21.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | train_loss:  0.0511 | valid_loss:  0.1684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [04:10<25:37, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | train_loss:  0.0476 | valid_loss:  0.1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [04:20<21:59, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | train_loss:  0.0487 | valid_loss:  0.1815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [04:31<19:35, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | train_loss:  0.0493 | valid_loss:  0.1738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [04:41<17:48, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | train_loss:  0.0492 | valid_loss:  0.1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [05:09<23:32, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | train_loss:  0.0496 | valid_loss:  0.1742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [05:33<25:30, 19.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | train_loss:  0.0496 | valid_loss:  0.1639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [05:44<21:53, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | train_loss:  0.0486 | valid_loss:  0.1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [05:55<19:19, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | train_loss:  0.0488 | valid_loss:  0.1882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [06:05<17:26, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | train_loss:  0.0480 | valid_loss:  0.1726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [06:16<15:56, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | train_loss:  0.0472 | valid_loss:  0.2066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [06:26<14:54, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | train_loss:  0.0483 | valid_loss:  0.1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [06:54<20:35, 16.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | train_loss:  0.0490 | valid_loss:  0.1703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [07:34<28:17, 23.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | train_loss:  0.0478 | valid_loss:  0.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [08:13<33:27, 28.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | train_loss:  0.0474 | valid_loss:  0.1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [08:50<36:02, 30.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | train_loss:  0.0493 | valid_loss:  0.1639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [09:26<37:29, 32.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | train_loss:  0.0474 | valid_loss:  0.1581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [09:51<34:21, 30.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | train_loss:  0.0471 | valid_loss:  0.1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [10:02<27:15, 24.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | train_loss:  0.0497 | valid_loss:  0.1660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [10:13<22:25, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | train_loss:  0.0479 | valid_loss:  0.1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [10:24<19:07, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | train_loss:  0.0480 | valid_loss:  0.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [10:34<16:23, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | train_loss:  0.0481 | valid_loss:  0.1821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [10:45<14:34, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | train_loss:  0.0485 | valid_loss:  0.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [10:55<13:10, 12.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | train_loss:  0.0473 | valid_loss:  0.1721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [11:05<12:17, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | train_loss:  0.0481 | valid_loss:  0.1905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [11:16<11:42, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | train_loss:  0.0478 | valid_loss:  0.1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [11:27<11:17, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | train_loss:  0.0476 | valid_loss:  0.1832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [11:38<10:46, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | train_loss:  0.0481 | valid_loss:  0.1778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [11:39<16:05, 16.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 126\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m#total_train_acc = 0\u001b[39;00m\n\u001b[0;32m    124\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m    127\u001b[0m     X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    128\u001b[0m     y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[88], line 50\u001b[0m, in \u001b[0;36mDiamondsDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 50\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# .values 판다스를 numpy로 바꿔줌\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx]\u001b[38;5;241m.\u001b[39mprice])\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m# tensor는 단순 float을 tensor로 바꿔주는데  []안에 넣어야함\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m : X,\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m : y,\n\u001b[0;32m     56\u001b[0m     }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# seed\n",
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# hyperparameter\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 1e-2\n",
    "hidden_dim = 32\n",
    "\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "\n",
    "label_encoders = {}\n",
    "for column in ('cut', 'color', 'clarity'):\n",
    "    label_encoder = LabelEncoder()\n",
    "    diamonds.loc[:, column] = label_encoder.fit_transform(diamonds[column])\n",
    "\n",
    "    label_encoders.update({column: label_encoder})\n",
    "\n",
    "\n",
    "diamonds = diamonds.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "train, temp = train_test_split(diamonds, test_size=0.4, random_state=seed)\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "train.loc[:, ['carat', 'depth', 'table', 'price','x', 'y', 'z']] = standard_scaler.fit_transform(train.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
    "valid.loc[:, ['carat', 'depth', 'table', 'price','x', 'y', 'z']] = standard_scaler.transform(valid.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
    "test.loc[:, ['carat', 'depth', 'table', 'price','x', 'y', 'z']] = standard_scaler.transform(test.loc[:, ['carat', 'depth', 'table', 'price',\t'x', 'y', 'z']])\n",
    "\n",
    "class DiamondsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.from_numpy(self.data.iloc[idx].drop('price').values).float() # .values 판다스를 numpy로 바꿔줌\n",
    "        y = torch.tensor([self.data.iloc[idx].price]).float() # tensor는 단순 float을 tensor로 바꿔주는데  []안에 넣어야함\n",
    "\n",
    "        return {\n",
    "            'X' : X,\n",
    "            'y' : y,\n",
    "        }\n",
    "    \n",
    "\n",
    "train_dataset = DiamondsDataset(train)\n",
    "valid_dataset = DiamondsDataset(valid)\n",
    "test_dataset = DiamondsDataset(test)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Model(\n",
    "    input_dim= len(diamonds.columns)-1,\n",
    "    hidden_dim= hidden_dim,\n",
    "    output_dim= 1,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "#train_accs = []\n",
    "valid_losses = []\n",
    "#valid_accs = []\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    ##################### Train ###########################\n",
    "    total_train_loss = 0\n",
    "    #total_train_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for data in train_dataloader:\n",
    "        X = data.get('X').to(device)\n",
    "        y = data.get('y').to(device)\n",
    "        #y = y.squeeze()\n",
    "        \n",
    "        optimizer.zero_grad() # 기울기 초기화\n",
    "        output = model(X)\n",
    "        logit = F.relu(output)\n",
    "        train_loss = criterion(output, y) \n",
    "        train_loss.backward()\n",
    "        optimizer.step() # 파라미터 업데이트\n",
    "\n",
    "        #train_acc = (logit.argmax(dim=-1) == y).float().mean()\n",
    "        total_train_loss += train_loss\n",
    "        #total_train_acc += train_acc\n",
    "\n",
    "    mean_train_loss = total_train_loss /len(train_dataloader)\n",
    "    #mean_train_acc = total_train_acc /len(train_dataloader)\n",
    "    train_losses.append(mean_train_loss)\n",
    "    #train_accs.append(mean_train_acc)\n",
    "    \n",
    "    ##################### Validation #############################\n",
    "\n",
    "    total_valid_loss = 0\n",
    "    #total_valid_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # valid는 기울기를 안써서 없애줌줌\n",
    "        for data in valid_dataloader:\n",
    "            X = data.get('X').to(device)\n",
    "            y = data.get('y').to(device)\n",
    "            #y = y.squeeze() # [32, 1]에서 1을 날려줌 [32]\n",
    "    \n",
    "            output = model(X)\n",
    "            logit = F.relu(output)\n",
    "            valid_loss = criterion(output, y)\n",
    "\n",
    "            #valid_acc = (logit.argmax(dim=-1) == y).float().mean()\n",
    "            total_valid_loss += valid_loss\n",
    "            #total_valid_acc += valid_acc\n",
    "\n",
    "    mean_valid_loss = total_valid_loss /len(valid_dataloader)\n",
    "    #mean_valid_acc = total_valid_acc /len(valid_dataloader)\n",
    "    valid_losses.append(mean_valid_loss)\n",
    "    #valid_accs.append(mean_valid_acc)\n",
    "\n",
    "    print(f'Epoch: {epoch} | train_loss: {mean_train_loss: .4f} | valid_loss: {mean_valid_loss: .4f}'  )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 예측값과 실제값을 넣어 계산합니다.\n",
    "\n",
    "result = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        X = data.get('X').to(device)\n",
    "        y = data.get('y').to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        result.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13916\\610139878.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m df = pd.DataFrame({\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m'y_pred'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;34m'groun_truth'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprice\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     })\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstandard_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstandard_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'price'"
     ]
    }
   ],
   "source": [
    "y_pred = torch.concat(result).squeeze()\n",
    "df = pd.DataFrame({\n",
    "    'y_pred': y_pred,\n",
    "    'groun_truth': test.price[:-7],\n",
    "    })\n",
    "\n",
    "df = df * standard_scaler.scale_[3] + standard_scaler.mean_[3]\n",
    "df = df.reset_index(drop=True).reset_index()\n",
    "px.scatter(\n",
    "    df.sample(n = 1000),\n",
    "    x = 'index',\n",
    "    y = ['y_pred','groun_truth']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
       "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
       "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
       "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
       "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
       "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
       "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
       "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
       "888         0       3  female   NaN      1      2  23.4500        S   Third   \n",
       "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
       "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
       "\n",
       "       who  adult_male deck  embark_town alive  alone  \n",
       "0      man        True  NaN  Southampton    no  False  \n",
       "1    woman       False    C    Cherbourg   yes  False  \n",
       "2    woman       False  NaN  Southampton   yes   True  \n",
       "3    woman       False    C  Southampton   yes  False  \n",
       "4      man        True  NaN  Southampton    no   True  \n",
       "..     ...         ...  ...          ...   ...    ...  \n",
       "886    man        True  NaN  Southampton    no   True  \n",
       "887  woman       False    B  Southampton   yes   True  \n",
       "888  woman       False  NaN  Southampton    no  False  \n",
       "889    man        True    C    Cherbourg   yes   True  \n",
       "890    man        True  NaN   Queenstown    no   True  \n",
       "\n",
       "[891 rows x 15 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "titanic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'male'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[241], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m titanic \u001b[38;5;241m=\u001b[39m \u001b[43mtitanic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6639\u001b[0m     ]\n\u001b[0;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'male'"
     ]
    }
   ],
   "source": [
    "pclass\tsex\tage\tsibsp\tparch\tfare\tembarked\tclass\twho\tadult_male\tdeck\tembark_town\talive\talone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\33788908.py:47: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 2 2 2 2 1 1 2 2 2 1 2 2 1 2 2 2 2 0 0 0 2 2 0 1 0 1 2 2 0 1 0 0 2 2 2 0\n",
      " 2 1 2 0 0 2 1 0 1 0 1 2 0 1 2 0 0 2 2 2 0 0 0 0 1 1 0 1 2 2 0 0 1 0 2 2 1\n",
      " 2 2 0 1 0 0 1 0 0 1 0 1 0 0 2 2 2 0 1 0 0 2 2 1 0 2 2 1 2 0 0 1 0 1 2 2 2\n",
      " 2 2 1 1 0 1 2 2 2 2 0 2 2 1 2 0 2 0 0 0 1 2 2 0 0 1 1 0 0 1 0 2 0 0 2 2 2\n",
      " 2 1 2 0 0 1 2 2 2 2 1 2 2 2 2 0 0 2 1 2 2 1 2 2 2 2 0 0 1 0 1 0 2 2 1 2 2\n",
      " 1 0 2 0 0 2 2 2 1 2 2 1 2 2 2 2 2 2 0 0 0 0 1 0 0 1 2 2 2 2 2 0 2 0 2 1 1\n",
      " 1 1 2 2 2 2 0 2 1 2 1 2 0 0 2 2 2 2 2 0 2 1 2 2 0 0 0 1 2 1 1 0 2 2 0 1 1\n",
      " 2 2 0 0 2 2 2 1 0 0 0 2 2 2 2 1 2 1 0 0 2 2 1 2 2 1 0 0 0 1 2 2 0 0 0 2 0\n",
      " 1 0 0 2 0 2 2 2 1 0 0 2 2 2 2 2 2 2 2 2 2 1 2 0 2 2 1 2 2 2 1 2 2 1 0 2 0\n",
      " 0 2 2 2 1 2 2 1 1 1 1 2 0 2 0 1 2 2 2 1 2 2 2 2 1 2 2 2 2 2 0 1 2 2 2 2 2\n",
      " 2 2 0 2 1 1 1 0 2 0 2 1 1 2 2 1 1 1 0 2 0 0 2 2 2 0 2 2 0 0 0 2 0 2 0 1 1\n",
      " 0 2 0 2 0 1 2 0 0 1 0 2 0 2 2 0 0 0 2 1 0 0 2 2 2 2 1 0 0 2 2 2 2 2 2 2 1\n",
      " 2 2 2 0 1 2 2 1 2 2 2 0 2 2 2 1 2 2 1 2 1 1 2 0 2 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\33788908.py:47: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0\n",
      " 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1\n",
      " 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0\n",
      " 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
      " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1\n",
      " 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0\n",
      " 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
      " 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1\n",
      " 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
      " 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\33788908.py:47: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 2 6 6 0 6 2 6 6 6 1 5 1 2 6 6 6 1\n",
      " 6 6 6 3 1 6 6 4 6 6 6 6 3 6 6 6 3 6 6 6 1 1 2 1 6 5 4 6 6 6 3 2 6 4 6 6 6\n",
      " 6 6 6 6 2 1 6 1 0 6 1 6 2 6 6 6 6 6 6 4 2 6 6 6 1 6 6 6 6 6 1 6 2 6 6 6 6\n",
      " 6 6 6 6 0 6 6 6 6 6 3 6 6 3 6 6 6 2 1 6 6 6 6 4 3 6 6 4 3 6 4 6 0 2 5 6 6\n",
      " 6 6 6 2 1 5 6 6 6 6 6 6 6 6 6 2 6 6 6 6 6 4 6 6 6 6 1 4 6 1 5 1 6 6 6 6 6\n",
      " 6 6 6 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 4 6 6 6 0 6 6 6 6 6 6 6 3 6 2 6 6 6\n",
      " 6 6 6 6 4 6 4 6 6 6 6 6 0 2 6 6 6 6 6 3 6 6 6 6 6 3 2 6 4 6 6 4 6 6 2 6 6\n",
      " 6 6 4 2 6 6 6 6 6 1 2 6 6 6 6 6 6 6 3 1 6 6 6 6 6 6 2 2 6 6 6 6 2 3 1 6 2\n",
      " 6 1 2 6 2 5 6 6 6 3 3 6 6 6 6 6 6 6 6 6 6 6 6 3 6 6 6 6 6 6 6 6 6 6 4 6 2\n",
      " 6 6 6 6 5 6 5 6 3 6 6 6 6 6 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 4 6 6 6 6 6\n",
      " 6 6 1 6 6 6 6 0 6 3 6 6 6 6 6 6 6 6 2 6 1 6 6 6 6 2 5 6 1 2 0 6 0 6 1 6 6\n",
      " 2 6 2 6 2 6 6 6 6 6 1 6 3 6 6 4 2 3 6 6 4 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 1 5 6 6 6 6 6 6 1 6 6 6 6 6 6 6 6 6 6 6 2 6 6]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\33788908.py:54: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 2 2 2 1 2 1 2 1 0 0 2 2 1 2 1 1 0 2 2 2 2 2 1 1 0 2 2 1 2 1 2 1 2 2 2 0\n",
      " 2 0 2 2 2 1 2 2 0 1 2 2 1 0 2 0 0 0 2 1 2 0 2 0 0 1 2 2 2 0 0 2 2 2 2 1 0\n",
      " 2 2 2 2 1 0 2 2 0 2 1 2 1 2 2 1 2 0 0 2 2 2 1 0 2 2 1 2 2 1 1 1 1 2 2 0 2\n",
      " 2 1 1 2 2 2 0 2 0 2 2 2 1 1 1 1 1 0 2 0 2 1 1 1 0 0 2 2 2 0 0 2 2 0 2 2 2\n",
      " 1 2 0 2 2 2 2 1 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\33788908.py:54: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
      " 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0\n",
      " 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0\n",
      " 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1\n",
      " 0 1 0 0 1 1 1 1 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\33788908.py:54: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[6 7 7 7 7 7 7 7 7 7 2 7 7 7 7 3 7 2 7 7 7 6 7 7 7 2 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 1 7 7 7 7 7 7 7 7 7 7 7 0 7 2 3 4 7 7 7 7 7 3 1 7 7 7 7 3 1 7 7 7 7 7 1\n",
      " 7 7 7 7 7 2 7 7 7 7 7 7 7 7 7 7 7 4 1 7 7 7 5 3 7 7 7 7 7 7 7 7 7 7 7 1 7\n",
      " 7 7 7 5 7 7 2 7 2 7 6 7 4 7 7 7 7 4 7 1 7 7 7 7 7 2 7 7 7 3 2 7 7 7 7 7 7\n",
      " 7 7 1 7 7 7 7 7 7]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\33788908.py:61: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 2 2 2 2 2 2 0 2 1 0 2 2 0 2 0 2 2 2 0 2 2 1 2 2 0 0 2 2 2 0 0 2 1 2 0 2\n",
      " 2 2 2 0 1 0 1 2 0 0 2 2 2 2 1 2 0 0 0 2 2 2 2 2 2 0 2 0 1 0 1 1 1 2 2 0 1\n",
      " 2 1 2 2 2 2 1 0 0 0 1 0 2 2 2 1 1 2 0 2 2 2 0 0 1 0 2 2 2 0 2 2 2 2 2 0 2\n",
      " 0 2 2 0 0 2 2 2 0 2 1 0 2 2 2 2 2 2 0 2 1 1 2 0 2 0 0 2 1 2 0 2 0 1 1 2 2\n",
      " 1 2 0 2 2 0 0 2 0]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\33788908.py:61: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0\n",
      " 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0\n",
      " 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0\n",
      " 1 0 1 0 1 0 0 1 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\33788908.py:61: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[7 7 4 7 7 7 7 3 7 7 7 7 7 0 7 7 7 7 7 3 7 7 7 7 7 7 2 7 7 7 1 7 7 7 7 3 7\n",
      " 7 7 7 1 7 1 4 7 0 3 7 7 7 7 7 7 7 2 2 7 7 7 7 7 7 2 7 4 7 1 5 7 7 7 7 1 7\n",
      " 7 7 7 7 7 7 7 1 7 2 7 2 7 7 7 3 7 7 1 7 7 7 2 4 7 1 7 7 7 7 7 7 7 7 7 2 7\n",
      " 3 7 7 4 4 7 7 7 2 7 7 7 6 7 7 7 7 7 2 7 7 7 7 4 7 1 7 7 7 7 2 7 1 7 7 7 7\n",
      " 7 7 0 7 7 2 2 7 0]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# seed\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# hyperparameter\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "hidden1_dim = 32\n",
    "\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic['class_'] = titanic['class']\n",
    "titanic = titanic.drop('class', axis=1)\n",
    "titanic.age = titanic.age.fillna(titanic.age.mean())\n",
    "titanic.deck = titanic.deck.fillna({'Nan':'N'})\n",
    "titanic.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "titanic = titanic.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "titanic = titanic.drop(columns=['alive', 'alone'])\n",
    "\n",
    "train, temp = train_test_split(titanic, test_size=0.4, random_state=seed)\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "train.loc[:, ['age', 'fare']]= standard_scaler.fit_transform(train.loc[:, ['age', 'fare']])\n",
    "valid.loc[:, ['age', 'fare']]= standard_scaler.transform(valid.iloc[:, 1:].loc[:, ['age', 'fare']])\n",
    "test.loc[:, ['age', 'fare']]= standard_scaler.transform(test.loc[:, ['age', 'fare']])\n",
    "\n",
    "\n",
    "label_encoders = {}\n",
    "for column in ('sex', 'embarked', 'class_', 'who', 'adult_male', 'deck', 'embark_town'):\n",
    "    label_encoder = LabelEncoder()\n",
    "    train.loc[:, column] = label_encoder.fit_transform(train[column])\n",
    "\n",
    "    label_encoders.update({column: label_encoder})\n",
    "\n",
    "label_encoders = {}\n",
    "for column in ('sex', 'embarked', 'class_', 'who', 'adult_male', 'deck', 'embark_town'):\n",
    "    label_encoder = LabelEncoder()\n",
    "    valid.loc[:, column] = label_encoder.fit_transform(valid[column])\n",
    "\n",
    "    label_encoders.update({column: label_encoder})\n",
    "\n",
    "label_encoders = {}\n",
    "for column in ('sex', 'embarked', 'class_', 'who', 'adult_male', 'deck', 'embark_town'):\n",
    "    label_encoder = LabelEncoder()\n",
    "    test.loc[:, column] = label_encoder.fit_transform(test[column])\n",
    "\n",
    "    label_encoders.update({column: label_encoder})\n",
    "\n",
    "\n",
    "train = train.astype(np.float32)\n",
    "valid = valid.astype(np.float32)\n",
    "test = test.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.from_numpy(self.data.iloc[idx].drop('survived').values).float()# .values 판다스를 numpy로 바꿔줌\n",
    "        y = torch.tensor([self.data.iloc[idx].survived]).float() # tensor는 단순 float을 tensor로 바꿔주는데  []안에 넣어야함\n",
    "\n",
    "        return {\n",
    "            'X' : X,\n",
    "            'y' : y,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TitanicDataset(train)\n",
    "valid_dataset = TitanicDataset(valid)\n",
    "test_dataset = TitanicDataset(test)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden1_dim = hidden1_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden1_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden1_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Model(\n",
    "    input_dim= len(titanic.columns)-1,\n",
    "    hidden1_dim= hidden1_dim,\n",
    "    output_dim= 1,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss:  0.6883 | train_acc:  59.15% | valid_loss:  0.6622 | valid_acc:  59.38%\n",
      "Epoch: 2 | train_loss:  0.6515 | train_acc:  58.93% | valid_loss:  0.6512 | valid_acc:  59.38%\n",
      "Epoch: 3 | train_loss:  0.6377 | train_acc:  57.81% | valid_loss:  0.6426 | valid_acc:  59.38%\n",
      "Epoch: 4 | train_loss:  0.6183 | train_acc:  58.93% | valid_loss:  0.6302 | valid_acc:  59.38%\n",
      "Epoch: 5 | train_loss:  0.6037 | train_acc:  58.26% | valid_loss:  0.6177 | valid_acc:  59.38%\n",
      "Epoch: 6 | train_loss:  0.5856 | train_acc:  59.60% | valid_loss:  0.6040 | valid_acc:  59.38%\n",
      "Epoch: 7 | train_loss:  0.5753 | train_acc:  58.04% | valid_loss:  0.5913 | valid_acc:  59.38%\n",
      "Epoch: 8 | train_loss:  0.5604 | train_acc:  58.48% | valid_loss:  0.5785 | valid_acc:  59.38%\n",
      "Epoch: 9 | train_loss:  0.5472 | train_acc:  59.15% | valid_loss:  0.5644 | valid_acc:  59.38%\n",
      "Epoch: 10 | train_loss:  0.5380 | train_acc:  59.15% | valid_loss:  0.5531 | valid_acc:  59.38%\n",
      "Epoch: 11 | train_loss:  0.5239 | train_acc:  59.15% | valid_loss:  0.5407 | valid_acc:  59.38%\n",
      "Epoch: 12 | train_loss:  0.5162 | train_acc:  58.26% | valid_loss:  0.5309 | valid_acc:  59.38%\n",
      "Epoch: 13 | train_loss:  0.5000 | train_acc:  59.15% | valid_loss:  0.5148 | valid_acc:  59.38%\n",
      "Epoch: 14 | train_loss:  0.4946 | train_acc:  58.71% | valid_loss:  0.5082 | valid_acc:  59.38%\n",
      "Epoch: 15 | train_loss:  0.4837 | train_acc:  58.48% | valid_loss:  0.5003 | valid_acc:  59.38%\n",
      "Epoch: 16 | train_loss:  0.4837 | train_acc:  58.93% | valid_loss:  0.4918 | valid_acc:  59.38%\n",
      "Epoch: 17 | train_loss:  0.4741 | train_acc:  58.26% | valid_loss:  0.4856 | valid_acc:  59.38%\n",
      "Epoch: 18 | train_loss:  0.4746 | train_acc:  59.15% | valid_loss:  0.4810 | valid_acc:  59.38%\n",
      "Epoch: 19 | train_loss:  0.4659 | train_acc:  58.26% | valid_loss:  0.4774 | valid_acc:  59.38%\n",
      "Epoch: 20 | train_loss:  0.4678 | train_acc:  58.48% | valid_loss:  0.4722 | valid_acc:  59.38%\n",
      "Epoch: 21 | train_loss:  0.4602 | train_acc:  58.26% | valid_loss:  0.4695 | valid_acc:  59.38%\n",
      "Epoch: 22 | train_loss:  0.4554 | train_acc:  58.04% | valid_loss:  0.4672 | valid_acc:  59.38%\n",
      "Epoch: 23 | train_loss:  0.4620 | train_acc:  58.71% | valid_loss:  0.4638 | valid_acc:  59.38%\n",
      "Epoch: 24 | train_loss:  0.4494 | train_acc:  58.48% | valid_loss:  0.4641 | valid_acc:  59.38%\n",
      "Epoch: 25 | train_loss:  0.4405 | train_acc:  58.71% | valid_loss:  0.4608 | valid_acc:  59.38%\n",
      "Epoch: 26 | train_loss:  0.4441 | train_acc:  59.15% | valid_loss:  0.4594 | valid_acc:  59.38%\n",
      "Epoch: 27 | train_loss:  0.4494 | train_acc:  58.04% | valid_loss:  0.4574 | valid_acc:  59.38%\n",
      "Epoch: 28 | train_loss:  0.4474 | train_acc:  58.48% | valid_loss:  0.4569 | valid_acc:  59.38%\n",
      "Epoch: 29 | train_loss:  0.4454 | train_acc:  58.48% | valid_loss:  0.4569 | valid_acc:  59.38%\n",
      "Epoch: 30 | train_loss:  0.4425 | train_acc:  59.38% | valid_loss:  0.4550 | valid_acc:  59.38%\n",
      "Epoch: 31 | train_loss:  0.4389 | train_acc:  58.71% | valid_loss:  0.4539 | valid_acc:  59.38%\n",
      "Epoch: 32 | train_loss:  0.4488 | train_acc:  59.15% | valid_loss:  0.4562 | valid_acc:  59.38%\n",
      "Epoch: 33 | train_loss:  0.4444 | train_acc:  58.71% | valid_loss:  0.4510 | valid_acc:  59.38%\n",
      "Epoch: 34 | train_loss:  0.4362 | train_acc:  59.82% | valid_loss:  0.4506 | valid_acc:  59.38%\n",
      "Epoch: 35 | train_loss:  0.4366 | train_acc:  58.04% | valid_loss:  0.4498 | valid_acc:  59.38%\n",
      "Epoch: 36 | train_loss:  0.4314 | train_acc:  59.15% | valid_loss:  0.4481 | valid_acc:  59.38%\n",
      "Epoch: 37 | train_loss:  0.4242 | train_acc:  59.15% | valid_loss:  0.4492 | valid_acc:  59.38%\n",
      "Epoch: 38 | train_loss:  0.4379 | train_acc:  58.93% | valid_loss:  0.4465 | valid_acc:  59.38%\n",
      "Epoch: 39 | train_loss:  0.4368 | train_acc:  57.81% | valid_loss:  0.4463 | valid_acc:  59.38%\n",
      "Epoch: 40 | train_loss:  0.4231 | train_acc:  58.48% | valid_loss:  0.4461 | valid_acc:  59.38%\n",
      "Epoch: 41 | train_loss:  0.4200 | train_acc:  58.71% | valid_loss:  0.4464 | valid_acc:  59.38%\n",
      "Epoch: 42 | train_loss:  0.4236 | train_acc:  59.15% | valid_loss:  0.4444 | valid_acc:  59.38%\n",
      "Epoch: 43 | train_loss:  0.4375 | train_acc:  58.93% | valid_loss:  0.4430 | valid_acc:  59.38%\n",
      "Epoch: 44 | train_loss:  0.4267 | train_acc:  58.71% | valid_loss:  0.4433 | valid_acc:  59.38%\n",
      "Epoch: 45 | train_loss:  0.4292 | train_acc:  59.38% | valid_loss:  0.4423 | valid_acc:  59.38%\n",
      "Epoch: 46 | train_loss:  0.4223 | train_acc:  58.04% | valid_loss:  0.4416 | valid_acc:  59.38%\n",
      "Epoch: 47 | train_loss:  0.4259 | train_acc:  58.93% | valid_loss:  0.4410 | valid_acc:  59.38%\n",
      "Epoch: 48 | train_loss:  0.4164 | train_acc:  58.93% | valid_loss:  0.4407 | valid_acc:  59.38%\n",
      "Epoch: 49 | train_loss:  0.4177 | train_acc:  59.38% | valid_loss:  0.4419 | valid_acc:  59.38%\n",
      "Epoch: 50 | train_loss:  0.4298 | train_acc:  58.48% | valid_loss:  0.4399 | valid_acc:  59.38%\n",
      "Epoch: 51 | train_loss:  0.4185 | train_acc:  59.82% | valid_loss:  0.4389 | valid_acc:  59.38%\n",
      "Epoch: 52 | train_loss:  0.4183 | train_acc:  58.93% | valid_loss:  0.4396 | valid_acc:  59.38%\n",
      "Epoch: 53 | train_loss:  0.4214 | train_acc:  58.48% | valid_loss:  0.4386 | valid_acc:  59.38%\n",
      "Epoch: 54 | train_loss:  0.4208 | train_acc:  59.60% | valid_loss:  0.4379 | valid_acc:  59.38%\n",
      "Epoch: 55 | train_loss:  0.4093 | train_acc:  58.93% | valid_loss:  0.4373 | valid_acc:  59.38%\n",
      "Epoch: 56 | train_loss:  0.4203 | train_acc:  58.48% | valid_loss:  0.4372 | valid_acc:  59.38%\n",
      "Epoch: 57 | train_loss:  0.4180 | train_acc:  57.37% | valid_loss:  0.4372 | valid_acc:  59.38%\n",
      "Epoch: 58 | train_loss:  0.4161 | train_acc:  59.15% | valid_loss:  0.4360 | valid_acc:  59.38%\n",
      "Epoch: 59 | train_loss:  0.4137 | train_acc:  60.04% | valid_loss:  0.4349 | valid_acc:  59.38%\n",
      "Epoch: 60 | train_loss:  0.4060 | train_acc:  58.71% | valid_loss:  0.4344 | valid_acc:  59.38%\n",
      "Epoch: 61 | train_loss:  0.4178 | train_acc:  59.15% | valid_loss:  0.4348 | valid_acc:  59.38%\n",
      "Epoch: 62 | train_loss:  0.4227 | train_acc:  58.71% | valid_loss:  0.4340 | valid_acc:  59.38%\n",
      "Epoch: 63 | train_loss:  0.4098 | train_acc:  59.15% | valid_loss:  0.4323 | valid_acc:  59.38%\n",
      "Epoch: 64 | train_loss:  0.4232 | train_acc:  58.93% | valid_loss:  0.4325 | valid_acc:  59.38%\n",
      "Epoch: 65 | train_loss:  0.4097 | train_acc:  58.48% | valid_loss:  0.4317 | valid_acc:  59.38%\n",
      "Epoch: 66 | train_loss:  0.4152 | train_acc:  58.26% | valid_loss:  0.4325 | valid_acc:  59.38%\n",
      "Epoch: 67 | train_loss:  0.4118 | train_acc:  59.60% | valid_loss:  0.4316 | valid_acc:  59.38%\n",
      "Epoch: 68 | train_loss:  0.3971 | train_acc:  59.60% | valid_loss:  0.4324 | valid_acc:  59.38%\n",
      "Epoch: 69 | train_loss:  0.4110 | train_acc:  58.04% | valid_loss:  0.4321 | valid_acc:  59.38%\n",
      "Epoch: 70 | train_loss:  0.4160 | train_acc:  59.38% | valid_loss:  0.4312 | valid_acc:  59.38%\n",
      "Epoch: 71 | train_loss:  0.4108 | train_acc:  58.93% | valid_loss:  0.4309 | valid_acc:  59.38%\n",
      "Epoch: 72 | train_loss:  0.4053 | train_acc:  58.26% | valid_loss:  0.4310 | valid_acc:  59.38%\n",
      "Epoch: 73 | train_loss:  0.4187 | train_acc:  58.26% | valid_loss:  0.4323 | valid_acc:  59.38%\n",
      "Epoch: 74 | train_loss:  0.4082 | train_acc:  58.26% | valid_loss:  0.4295 | valid_acc:  59.38%\n",
      "Epoch: 75 | train_loss:  0.4051 | train_acc:  58.04% | valid_loss:  0.4303 | valid_acc:  59.38%\n",
      "Epoch: 76 | train_loss:  0.4078 | train_acc:  59.15% | valid_loss:  0.4295 | valid_acc:  59.38%\n",
      "Epoch: 77 | train_loss:  0.4083 | train_acc:  58.04% | valid_loss:  0.4291 | valid_acc:  59.38%\n",
      "Epoch: 78 | train_loss:  0.4057 | train_acc:  59.15% | valid_loss:  0.4293 | valid_acc:  59.38%\n",
      "Epoch: 79 | train_loss:  0.4068 | train_acc:  58.26% | valid_loss:  0.4300 | valid_acc:  59.38%\n",
      "Epoch: 80 | train_loss:  0.4081 | train_acc:  58.93% | valid_loss:  0.4286 | valid_acc:  59.38%\n",
      "Epoch: 81 | train_loss:  0.4092 | train_acc:  58.71% | valid_loss:  0.4278 | valid_acc:  59.38%\n",
      "Epoch: 82 | train_loss:  0.4052 | train_acc:  58.48% | valid_loss:  0.4282 | valid_acc:  59.38%\n",
      "Epoch: 83 | train_loss:  0.4060 | train_acc:  58.04% | valid_loss:  0.4285 | valid_acc:  59.38%\n",
      "Epoch: 84 | train_loss:  0.4064 | train_acc:  59.60% | valid_loss:  0.4274 | valid_acc:  59.38%\n",
      "Epoch: 85 | train_loss:  0.4125 | train_acc:  58.26% | valid_loss:  0.4267 | valid_acc:  59.38%\n",
      "Epoch: 86 | train_loss:  0.4095 | train_acc:  59.60% | valid_loss:  0.4273 | valid_acc:  59.38%\n",
      "Epoch: 87 | train_loss:  0.4048 | train_acc:  59.15% | valid_loss:  0.4269 | valid_acc:  59.38%\n",
      "Epoch: 88 | train_loss:  0.4122 | train_acc:  58.26% | valid_loss:  0.4269 | valid_acc:  59.38%\n",
      "Epoch: 89 | train_loss:  0.3919 | train_acc:  60.04% | valid_loss:  0.4280 | valid_acc:  59.38%\n",
      "Epoch: 90 | train_loss:  0.4103 | train_acc:  59.15% | valid_loss:  0.4255 | valid_acc:  59.38%\n",
      "Epoch: 91 | train_loss:  0.4005 | train_acc:  58.71% | valid_loss:  0.4255 | valid_acc:  59.38%\n",
      "Epoch: 92 | train_loss:  0.4092 | train_acc:  59.15% | valid_loss:  0.4250 | valid_acc:  59.38%\n",
      "Epoch: 93 | train_loss:  0.4028 | train_acc:  58.48% | valid_loss:  0.4258 | valid_acc:  59.38%\n",
      "Epoch: 94 | train_loss:  0.4018 | train_acc:  58.04% | valid_loss:  0.4262 | valid_acc:  59.38%\n",
      "Epoch: 95 | train_loss:  0.4024 | train_acc:  58.48% | valid_loss:  0.4260 | valid_acc:  59.38%\n",
      "Epoch: 96 | train_loss:  0.3962 | train_acc:  59.38% | valid_loss:  0.4262 | valid_acc:  59.38%\n",
      "Epoch: 97 | train_loss:  0.3900 | train_acc:  58.93% | valid_loss:  0.4255 | valid_acc:  59.38%\n",
      "Epoch: 98 | train_loss:  0.4042 | train_acc:  58.04% | valid_loss:  0.4280 | valid_acc:  59.38%\n",
      "Epoch: 99 | train_loss:  0.4084 | train_acc:  58.71% | valid_loss:  0.4256 | valid_acc:  59.38%\n",
      "Epoch: 100 | train_loss:  0.4117 | train_acc:  59.15% | valid_loss:  0.4247 | valid_acc:  59.38%\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    ##################### Train ###########################\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for data in train_dataloader:\n",
    "        X = data.get('X').to(device)\n",
    "        y = data.get('y').to(device)\n",
    "        # y = y.squeeze()\n",
    "        \n",
    "        optimizer.zero_grad() # 기울기 초기화\n",
    "        output = model(X)\n",
    "        train_loss = criterion(output, y) \n",
    "        train_loss.backward()\n",
    "        optimizer.step() # 파라미터 업데이트\n",
    "\n",
    "        train_acc = (output.argmax(dim=-1) == y).float().mean()\n",
    "        total_train_loss += train_loss\n",
    "        total_train_acc += train_acc\n",
    "\n",
    "    mean_train_loss = total_train_loss /len(train_dataloader)\n",
    "    mean_train_acc = total_train_acc /len(train_dataloader)\n",
    "    train_losses.append(mean_train_loss)\n",
    "    train_accs.append(mean_train_acc)\n",
    "    \n",
    "    ##################### Validation #############################\n",
    "\n",
    "    total_valid_loss = 0\n",
    "    total_valid_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # valid는 기울기를 안써서 없애줌줌\n",
    "        for data in valid_dataloader:\n",
    "            X = data.get('X').to(device)\n",
    "            y = data.get('y').to(device)\n",
    "            #y = y.squeeze() # [32, 1]에서 1을 날려줌 [32]\n",
    "    \n",
    "            output = model(X)\n",
    "            valid_loss = criterion(output, y)\n",
    "\n",
    "            valid_acc = (output.argmax(dim=-1) == y).float().mean()\n",
    "            total_valid_loss += valid_loss\n",
    "            total_valid_acc += valid_acc\n",
    "\n",
    "        mean_valid_loss = total_valid_loss /len(valid_dataloader)\n",
    "        mean_valid_acc = total_valid_acc /len(valid_dataloader)\n",
    "        valid_losses.append(mean_valid_loss)\n",
    "        valid_accs.append(mean_valid_acc)\n",
    "\n",
    "        print(f'Epoch: {epoch} | train_loss: {mean_train_loss: .4f} | train_acc: {mean_train_acc*100: .2f}% | valid_loss: {mean_valid_loss: .4f} | valid_acc: {mean_valid_acc*100: .2f}%'  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4192, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0455,  0.6506],\n",
       "        [-0.1160,  0.7964],\n",
       "        [ 0.1682,  0.5028],\n",
       "        [ 0.1514,  0.5548],\n",
       "        [-0.1862,  0.1890],\n",
       "        [-0.3271,  0.3104],\n",
       "        [-0.6174,  0.6595],\n",
       "        [-0.1333,  0.7524],\n",
       "        [-0.0140,  0.4195],\n",
       "        [-0.0430,  0.3491],\n",
       "        [ 0.0371,  0.6218],\n",
       "        [-0.0084,  0.5185],\n",
       "        [    nan,     nan],\n",
       "        [ 0.1127,  0.4422],\n",
       "        [-0.3351,  0.5617],\n",
       "        [-0.3079,  0.6554],\n",
       "        [ 0.3996,  0.4888],\n",
       "        [-0.0080,  0.5189],\n",
       "        [ 0.1136,  0.3526],\n",
       "        [    nan,     nan],\n",
       "        [-0.4009,  0.7007],\n",
       "        [-0.0177,  0.5490],\n",
       "        [ 0.1759,  0.4964],\n",
       "        [    nan,     nan],\n",
       "        [-0.2737,  0.2873],\n",
       "        [ 0.0559,  0.6829],\n",
       "        [-0.0750,  0.7060],\n",
       "        [ 0.0861,  0.3349],\n",
       "        [ 0.0152,  0.0913],\n",
       "        [ 0.0080,  0.6350],\n",
       "        [-0.0695,  0.5461],\n",
       "        [-0.0400,  0.5186]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4003335231.py:27: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 0 2 0 2 2 0 2 2 1 2 0 2 2 2 1 2 1 2 2 1 1 2 0 2 2 2 0 2 2 0 0 2 1 0 0 2\n",
      " 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 0 1 0 0 1 2 1 2 2 0 0 2 0 2 1 2 2 2 1 2 1 2\n",
      " 2 2 2 2 1 2 2 2 2 0 1 2 2 2 0 2 2 2 0 2 2 2 0 0 1 1 2 2 0 2 2 2 2 2 2 2 0\n",
      " 2 2 2 2 2 2 1 0 2 1 2 1 1 0 2 2 2 2 2 2 2 2 1 1 1 0 0 2 0 2 2 2 2 1 1 2 2\n",
      " 1 1 1 0 2 2 2 0 2 2 2 2 2 1 2 2 2 2 0 2 0 2 0 2 2 2 0 2 2 0 1 2 2 1 2 1 2\n",
      " 0 2 0 2 2 1 1 2 1 0 0 2 2 2 1 2 2 2 2 2 2 2 2 2 0 2 1 2 1 2 0 2 1 0 1 2 1\n",
      " 2 2 0 2 1 2 1 2 0 2 1 2 1 2 1 1 1 1 2 2 1 2 2 0 2 1 0 1 2 2 0 2 2 2 0 0 0\n",
      " 1 2 2 0 0 2 1 2 2 0 0 0 2 1 0 2 0 2 1 2 2 2 2 2 2 0 2 2 2 1 2 0 0 1 2 2 0\n",
      " 2 0 0 0 2 2 2 1 2 0 0 0 1 0 0 0 1 2 1 2 1 1 0 0 2 2 1 1 2 0 2 1 2 0 2 0 0\n",
      " 2 0 2 0 0 2 0 1 0 1 1 1 1 1 2 2 2 2 0 2 2 2 2 0 1 2 2 2 1 2 2 2 2 0 2 2 0\n",
      " 0 2 2 0 2 0 2 0 2 2 0 2 2 0 2 1 2 1 2 1 0 2 2 0 2 2 2 1 1 1 2 2 2 2 2 1 2\n",
      " 1 2 2 2 2 0 1 2 2 1 1 1 2 2 2 2 2 2 2 1 1 2 2 0 2 1 2 0 0 2 1 0 1 1 2 2 1\n",
      " 2 0 1 0 2 0 1 2 0 0 2 2 0 0 1 2 0 2 0 1 2 2 1 0 2 2 2 2 1 1 2 0 1 2 2 2 2\n",
      " 1 2 2 0 2 0 0 2 2 2 2 0 0 2 2 0 2 0 2 2 2 2 2 0 0 1 0 2 2 2 2 0 0 2 0 1 2\n",
      " 1 2 0 2 2 0 2 2 1 0 2 1 1 2 2 2 2 1 0 0 2 0 0 2 2 1 0 0 1 1 2 1 0 1 2 2 2\n",
      " 0 0 0 0 2 2 2 1 2 2 2 2 2 2 2 1 0 0 2 2 2 1 0 2 2 1 0 1 0 2 0 1 0 2 2 2 0\n",
      " 2 2 1 2 1 2 2 0 1 2 0 2 0 2 2 0 1 0 2 2 2 2 2 1 2 2 1 1 2 0 2 2 2 0 1 0 2\n",
      " 2 0 2 0 0 2 1 2 1 2 2 2 0 2 2 2 0 2 0 2 2 2 1 2 2 2 1 2 2 1 0 0 2 0 2 2 1\n",
      " 1 2 2 0 1 0 1 1 1 2 2 2 2 0 2 0 2 2 1 1 2 2 2 0 0 2 2 2 0 1 2 2 0 2 0 0 2\n",
      " 2 2 1 1 0 0 2 0 0 0 2 1 2 0 1 2 2 1 2 1 1 0 2 1 2 1 2 0 2 1 1 1 2 2 0 2 2\n",
      " 0 0 0 2 2 0 2 1 0 2 1 2 2 2 1 1 2 1 2 0 2 2 2 0 2 0 0 2 2 2 2 2 1 2 1 2 2\n",
      " 2 2 0 2 0 0 2 2 2 2 2 2 0 2 1 2 0 2 1 0 2 2 2 1 1 0 2 2 2 0 2 1 0 2 2 1 2\n",
      " 2 0 2 1 2 2 0 2 0 2 2 2 2 1 2 0 2 1 2 2 2 0 2 2 2 0 2 1 0 2 2 2 2 2 1 0 2\n",
      " 2 2 0 1 2 0 0 2 2 2 1 0 2 1 1 1 0 2 2 2 0 0 2 1 2 2 2 2 0 1 2 2 1 2 2 1 0\n",
      " 2 0 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4003335231.py:27: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1\n",
      " 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1\n",
      " 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0\n",
      " 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n",
      " 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
      " 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1\n",
      " 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1\n",
      " 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0\n",
      " 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
      " 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0\n",
      " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
      " 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1\n",
      " 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0\n",
      " 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
      " 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0\n",
      " 0 1 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4003335231.py:27: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[7 2 7 2 7 7 4 7 7 7 6 2 7 7 7 7 7 7 7 7 7 3 7 0 7 7 7 2 7 7 7 1 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 1 2 7 7 7 7 7 1 2 7 7 7 5 7 7 7 7 7 7 7\n",
      " 7 5 7 7 7 7 7 7 7 7 7 7 7 7 2 7 7 7 4 7 7 7 0 3 7 7 7 7 3 7 7 7 7 7 7 7 2\n",
      " 7 7 7 7 7 7 7 1 7 7 7 7 4 3 7 7 7 5 7 7 7 7 7 7 7 3 2 7 1 7 7 7 7 7 7 7 7\n",
      " 5 7 7 2 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 7 7 7 1 7 7 7 0 7 7 2 7 7 7 7 7 5 7\n",
      " 0 7 7 7 7 7 7 7 5 1 1 7 7 7 7 7 7 7 7 7 6 7 7 7 0 7 7 7 7 7 3 7 7 3 7 7 7\n",
      " 7 7 2 7 7 7 7 7 2 7 7 7 7 7 7 7 7 7 7 7 7 7 7 2 7 7 3 7 7 6 2 7 7 7 7 1 7\n",
      " 7 7 7 4 1 7 7 7 7 2 2 7 7 7 2 7 3 7 7 7 7 7 7 7 7 0 7 7 7 7 7 7 1 3 7 7 7\n",
      " 7 2 2 1 7 7 7 4 7 2 7 2 7 4 2 1 7 7 7 7 7 7 2 4 7 7 7 7 7 2 7 3 7 1 7 2 2\n",
      " 7 7 7 2 4 7 7 5 2 7 7 7 5 7 7 7 7 7 2 7 7 7 7 4 7 7 7 7 7 7 7 7 7 3 7 7 1\n",
      " 4 7 7 7 7 7 7 2 7 7 7 7 7 7 7 7 7 7 7 7 1 7 7 3 6 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 2 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 2 7 7 7 4 1 7 7 2 7 7 7 7 7\n",
      " 7 0 7 7 7 2 7 7 2 2 7 7 4 3 7 7 4 7 4 7 7 7 7 7 7 7 7 7 7 3 7 0 7 7 7 7 7\n",
      " 7 7 7 1 7 2 1 7 7 7 7 2 7 7 7 3 7 2 7 7 7 7 7 1 2 7 7 7 7 7 7 4 7 7 3 5 7\n",
      " 7 7 1 7 7 1 7 7 7 2 7 7 7 7 7 7 7 7 1 7 7 1 1 7 7 7 2 7 7 7 7 7 2 7 7 7 7\n",
      " 7 0 7 4 7 7 7 7 7 7 7 7 7 7 7 7 2 4 7 7 7 7 4 7 7 7 2 7 0 7 4 7 1 7 7 7 3\n",
      " 7 7 7 7 7 7 7 0 7 7 7 7 7 7 7 7 7 2 7 7 7 7 7 7 7 7 5 7 7 3 7 7 7 3 7 3 7\n",
      " 7 0 7 1 7 7 7 7 7 7 7 7 1 7 7 7 3 7 0 7 7 7 7 7 7 7 7 7 7 7 3 7 7 4 7 7 7\n",
      " 7 7 7 2 7 1 7 7 7 7 7 7 7 1 7 3 7 7 7 7 7 7 7 1 1 7 7 7 7 7 7 7 2 5 2 4 7\n",
      " 7 7 7 7 4 7 7 2 2 2 7 7 5 2 4 7 7 7 7 7 7 4 7 7 7 7 7 1 7 7 7 7 7 7 1 7 7\n",
      " 3 2 1 7 7 1 7 7 3 7 7 4 7 7 7 7 7 7 7 1 7 7 7 1 7 3 7 7 7 7 7 7 4 7 7 7 5\n",
      " 7 7 1 7 1 3 7 7 7 7 7 7 1 7 7 7 7 7 7 3 7 7 7 7 7 1 7 7 7 0 7 7 4 7 7 7 7\n",
      " 7 1 7 7 7 7 1 7 7 4 7 7 7 7 7 1 7 7 7 7 7 4 7 7 7 2 7 7 7 7 7 7 7 7 7 2 7\n",
      " 7 7 3 7 7 7 4 7 7 7 7 3 7 7 7 7 0 7 7 7 3 1 7 7 7 7 7 7 2 7 7 7 7 7 7 7 1\n",
      " 7 2 7]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13916\\4003335231.py:27: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1\n",
      " 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1\n",
      " 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0\n",
      " 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0\n",
      " 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1\n",
      " 0 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0\n",
      " 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1\n",
      " 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1\n",
      " 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1\n",
      " 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 0\n",
      " 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1\n",
      " 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0\n",
      " 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0\n",
      " 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0\n",
      " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1\n",
      " 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0\n",
      " 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1\n",
      " 0 1 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# seed\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# hyperparameter\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 1e-2\n",
    "hidden_dim = 32\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic['class_'] = titanic['class']\n",
    "titanic = titanic.drop('class', axis=1)\n",
    "\n",
    "label_encoders = {}\n",
    "for column in ('sex', 'embarked', 'class_', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'):\n",
    "    label_encoder = LabelEncoder()\n",
    "    titanic.loc[:, column] = label_encoder.fit_transform(titanic[column])\n",
    "\n",
    "    label_encoders.update({column: label_encoder})\n",
    "\n",
    "\n",
    "\n",
    "titanic.sex = titanic.sex.astype(float)\n",
    "titanic.embarked = titanic.embarked.astype(float)\n",
    "titanic.class_ = titanic.class_.astype(float)\n",
    "titanic.who = titanic.who.astype(float)\n",
    "titanic.adult_male = titanic.adult_male.astype(float)\n",
    "titanic.deck = titanic.deck.astype(float)\n",
    "titanic.embark_town = titanic.embark_town.astype(float)\n",
    "titanic.alive = titanic.alive.astype(float)\n",
    "titanic.alone = titanic.alone.astype(float)\n",
    "\n",
    "\n",
    "titanic = titanic.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "train, temp = train_test_split(titanic, test_size=0.4, random_state=seed)\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "train = standard_scaler.fit_transform(train)\n",
    "valid = standard_scaler.transform(valid)\n",
    "test = standard_scaler.transform(test)\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.from_numpy(self.data.iloc[idx].drop('survived').values).float() # .values 판다스를 numpy로 바꿔줌\n",
    "        y = torch.tensor([self.data.iloc[idx].survived]).float() # tensor는 단순 float을 tensor로 바꿔주는데  []안에 넣어야함\n",
    "\n",
    "        return {\n",
    "            'X' : X,\n",
    "            'y' : y,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning\n",
    "\n",
    "PyTorch Lightning은 PyTorch의 강력함을 유지하면서도 코드의 재사용성과 관리 편의성을 크게 개선시킨 <br>\n",
    "주로 복잡한 훈련 루프를 추상화하여 코드의 간결성을 높이고, 디버깅과 스케일링을 용이하게 만드는 것을 주된 목표로 함 <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 20pt;\"> 장점 </span> \n",
    "\n",
    "1. 모듈화된 구조: PyTorch Lightning은 훈련, 검증, 테스트 루프와 옵티마이저, 데이터 로더 등을 각각의 모듈로 분리하여 제공 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> 훨씬 읽기 쉽고 관리하기 쉽게 만들어 줌 (코드의 간결성 확보)\n",
    "2. 자동화된 훈련 루프: LightningModule 클래스를 상속받아 모델을 정의할 때, 훈련 루프에서 필요한 많은 부분들(forward, loss, optimizer 등)이 자동으로 처리 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> 사용자가 코드의 일부를 더 간결하게 작성할 수 있도록 해줌\n",
    "3. 스케일링과 분산 훈련 지원: multi-GPU나 분산 훈련을 지원하여 더 큰 데이터셋이나 복잡한 모델을 쉽게 처리할 수 있게 해줌\n",
    "5. 확장성: 기존의 PyTorch 코드를 LightningModule로 변환하는 과정이 비교적 간단하고, 다양한 연구와 프로젝트에 쉽게 적용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "learning_rate = 1e-3\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningDataModule\n",
    "\n",
    "학습 시 사용할 DataLoader를 호출하는 클래스 <br>\n",
    "관련된 전처리 등을 내부에서 처리하여 사용 <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size:150%\">코드</span>\n",
    "\n",
    "> ```python\n",
    "> class CustomDataModule(L.LightningDataModule):\n",
    ">     def __init__(\n",
    ">         self,\n",
    ">         batch_size: int,\n",
    ">         num_workers: int,\n",
    ">         ):\n",
    ">         super().__init__()\n",
    ">         self.num_workers = 4\n",
    ">         self.batch_size = batch_size\n",
    "> \n",
    ">     def prepare(self, train, valid, test):     \n",
    ">         self.train, self.valid, self.test = train, valid, test\n",
    "> \n",
    ">     def setup(self, stage: str):\n",
    ">         if stage == \"fit\":      \n",
    ">             self.train_data = self.train\n",
    ">             self.valid_data = self.valid\n",
    "> \n",
    ">         if stage == \"test\":     \n",
    ">             self.test_data = self.test\n",
    "> \n",
    ">     def train_dataloader(self):\n",
    ">         return DataLoader(\n",
    ">             dataset=self.train_data,\n",
    ">             batch_size=self.batch_size,\n",
    ">             shuffle=False,\n",
    ">         )\n",
    "> \n",
    ">     def val_dataloader(self):\n",
    ">         return DataLoader(\n",
    ">             dataset=self.val_data,\n",
    ">             batch_size=self.batch_size,\n",
    ">             shuffle=False,\n",
    ">         )\n",
    "> \n",
    ">     def test_dataloader(self):\n",
    ">         return DataLoader(\n",
    ">             dataset=self.test_data,\n",
    ">             batch_size=self.batch_size,\n",
    ">             shuffle=False,\n",
    ">         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "         self,\n",
    "         batch_size: int,\n",
    "         ):\n",
    "         super().__init__()\n",
    "         self.num_workers = 4\n",
    "         self.batch_size = batch_size\n",
    " \n",
    "    def prepare(self, train_dataset, valid_dataset, test_dataset):     \n",
    "        self.train_dataset, self.valid_dataset, self.test_dataset = train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":      \n",
    "            self.train_data = self.train_dataset\n",
    "            self.valid_data = self.valid_dataset\n",
    "\n",
    "        if stage == \"test\":     \n",
    "            self.test_data = self.test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last = True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.valid_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last = True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last = True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningModule\n",
    "\n",
    "학습 과정에 사용될 코드를 모듈화 한 클래스 <br>\n",
    "self.log()를 통해 logging 사용 가능 \n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size:150%\">코드</span>\n",
    "\n",
    "> ```python\n",
    "> class LightningMLP(L.LightningModule):\n",
    ">     def __init__(\n",
    ">         self,\n",
    ">         model: torch.nn,    # 구축한 모델\n",
    ">         learning_rate: float,\n",
    ">         ):\n",
    ">         super().__init__()\n",
    ">         self.model = model\n",
    ">         self.learning_rate = learning_rate\n",
    ">     \n",
    ">     def training_step(self, batch, batch_idx):      # train 코드\n",
    ">         pass\n",
    ">     \n",
    ">     def on_train_epoch_end(self, *args, **kwargs):  # train이 1 epoch 끝날 때 실행될 코드, 주로 metric 출력을 위해 사용\n",
    ">         pass\n",
    ">     \n",
    ">     def validation_step(self, *args, **kwargs):     # validation 코드\n",
    ">         pass\n",
    ">     \n",
    ">     def on_validation_epoch_end(self):              # valid가 1 epoch 끝날 때 실행될 코드, 주로 metric 출력을 위해 사용\n",
    ">         pass\n",
    ">     \n",
    ">     def test_step(self, *args, **kwargs):           # test 코드\n",
    ">         pass\n",
    ">     \n",
    ">     def configure_optimizers(self):                 # optimizer 설정\n",
    ">         pass\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn,    # 구축한 모델\n",
    "        learning_rate: float,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.total_train_loss = []\n",
    "        self.total_train_acc = []\n",
    "        self.total_valid_loss = []\n",
    "        self.total_valid_acc = []\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):      # train 코드\n",
    "        if batch_idx == 0:\n",
    "            self.total_train_loss.clear()\n",
    "            self.total_train_acc.clear()\n",
    "\n",
    "        X = batch.get('X')\n",
    "        y = batch.get('y')\n",
    "        y = y.squeeze()\n",
    "\n",
    "        output = self.model(X)\n",
    "        logit = F.softmax(output, dim= -1)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "\n",
    "        predicted_label = logit.argmax(dim= -1)\n",
    "        acc = (predicted_label == y).float().mean()\n",
    "\n",
    "        self.total_train_loss.append(loss.item()) \n",
    "        self.total_train_acc.append(acc.item()) \n",
    "\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self, *args, **kwargs):  # train이 1 epoch 끝날 때 실행될 코드, 주로 metric 출력을 위해 사용\n",
    "        print(f'train_loss: {np.mean(self.total_train_loss): .4f} | train_acc: {np.mean(self.total_train_acc)*100: .2f}%')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):     # validation 코드\n",
    "        if batch_idx == 0:\n",
    "            self.total_valid_loss.clear()\n",
    "            self.total_valid_acc.clear()\n",
    "\n",
    "        X = batch.get('X')\n",
    "        y = batch.get('y')\n",
    "        y = y.squeeze()\n",
    "\n",
    "        output = self.model(X)\n",
    "        logit = F.softmax(output, dim= -1)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "\n",
    "        predicted_label = logit.argmax(dim= -1)\n",
    "        acc = (predicted_label == y).float().mean()\n",
    "\n",
    "        self.total_valid_loss.append(loss.item()) \n",
    "        self.total_valid_acc.append(acc.item()) \n",
    "\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):              # valid가 1 epoch 끝날 때 실행될 코드, 주로 metric 출력을 위해 사용\n",
    "        print(f'valid_loss: {np.mean(self.total_train_loss): .4f} | valid_acc: {np.mean(self.total_train_acc)*100: .2f}%')\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch):           # test 코드\n",
    "        X = batch.get('X')\n",
    "        y = batch.get('y')\n",
    "        y = y.squeeze()\n",
    "\n",
    "        output = self.model(X)\n",
    "        logit = F.softmax(output, dim= -1)\n",
    "\n",
    "        predicted_label = logit.argmax(dim= -1)\n",
    "\n",
    "        return predicted_label\n",
    "    \n",
    "    def configure_optimizers(self):                 # optimizer 설정\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr= self.learning_rate,\n",
    "        ) \n",
    "\n",
    "        return {'optimizer': optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_module = TitanicDataModule(batch_size)\n",
    "titanic_data_module.prepare(train_dataset, valid_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(len(train.columns)-1, 32, 2)\n",
    "titanic_module = TitanicModule(model, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=epoch,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type  | Params | Mode \n",
      "----------------------------------------\n",
      "0 | model | Model | 482    | train\n",
      "----------------------------------------\n",
      "482       Trainable params\n",
      "0         Non-trainable params\n",
      "482       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: PossibleUserWarning:\n",
      "\n",
      "The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[450], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtitanic_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtitanic_data_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1049\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    395\u001b[0m )\n\u001b[1;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[442], line 52\u001b[0m, in \u001b[0;36mTitanicModule.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     50\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X)\n\u001b[0;32m     51\u001b[0m logit \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m logit\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m acc \u001b[38;5;241m=\u001b[39m (predicted_label \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model = titanic_module,\n",
    "    datamodule = titanic_data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "학습에 필요한 내용을 정의하는 클래스\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size:150%\">코드</span>\n",
    "\n",
    "> ```python\n",
    "> trainer = Trainer(\n",
    ">     # accelerator='gpu',    # gpu 사용 시 \n",
    ">     # devices=2,            # gpu 사용 시 장치 위치 입력 ex) 0, 1, 2, [0, 1], [0, 3], ...\n",
    ">     max_epochs=epochs,\n",
    ">     callbacks=[             # callback\n",
    ">         EarlyStopping(monitor='val_loss', mode='min', patience=10)], # val_loss를 기준으로 최솟값이 10번 이상 업데이트되지 않으면 학습 중지\n",
    ">     log_every_n_steps=1,\n",
    "> )\n",
    "> ```\n",
    "\n",
    "<br>\n",
    "\n",
    "생성된 trainer 객체를 이용하여 학습 진행\n",
    "\n",
    "> ```python\n",
    "> trainer.fit(\n",
    ">     model=custom_model,\n",
    ">     datamodule=custom_data_module,\n",
    "> )\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "학습이 진행되면서 checpoint가 생성되는데, 이를 통해 학습된 모델 사용 가능\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size:150%\">코드</span>\n",
    "\n",
    "> ```python\n",
    "> model = CustomDataModule.load_from_checkpoint(path)\n",
    "> ```\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "학습된 모델을 가지고 테스트 데이터에 대해 아래와 같이 사용 가능\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size:150%\">코드</span>\n",
    "\n",
    "> ```python\n",
    "> predictions = trainer.predict(model: L.LightningModule, data_loader)\n",
    "> ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
