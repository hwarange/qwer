{"cells":[{"cell_type":"markdown","metadata":{"id":"GgI-MEwoR01T"},"source":["# Lightgbm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install bayesian-optimization"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import scipy as sp\n","\n","from sklearn.linear_model import LinearRegression, Lasso, Ridge\n","from sklearn.svm import SVC, SVR\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from bayes_opt import BayesianOptimization  # bayesian-optimization\n","\n","from sklearn.metrics import mean_squared_error, accuracy_score\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px"]},{"cell_type":"markdown","metadata":{"id":"nXnt1h2VR91E"},"source":["# Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{},"source":["## Manual Search\n","\n","- 직접 파라미터를 설정하는 직접 탐색 방법  \n","- 전체 범위를 보는 것이 아니기에 현재까지의 최적의 파라미터가 최적인지는 보장하지 못 함  \n","- 여러 종류의 파라미터를 동시에 탐색하기에는 한계"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"o_HEBYkfSBhM"},"source":["## Greed Search\n","\n","<img src=\"https://www.yourdatateacher.com/wp-content/uploads/2021/03/image-6.png\" width=\"400\" height=\"400\"/>\n","\n","- 탐색 구간 내 모든 hyperparameter 조합을 일정 구간으로 나누어 시도  \n","- 파라미터가 많아질수록 기하급수적으로 많은 시간 소요\n","\n","<br>\n","\n","- 사용 방법\n","> ```python\n","> from sklearn.model_selection import GridSearchCV\n","> parameters = {'parameter1': ('value1', 'value2', ...), 'parameter2':[value1, value2, ...]}\n","> grid_search = GridSearchCV(model, parameters)\n","> grid_search.fit(X, y)\n","> ```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"q8UfbTEpSDh8"},"source":["## Random Search\n","\n","<img src=\"https://www.yourdatateacher.com/wp-content/uploads/2021/03/image-7.png\" width=\"400\" height=\"400\"/>\n","\n","- 탐색 대상 구간 내의 hyperparameter를 랜덤 샘플링  \n","- Grid Search 대비 반복 횟수를 줄이는 동시에 확률적 탐색으로 **최적에 근사한 parameter를  빨리 찾을 수 있는 것으로 알려짐**  \n","- 다만, 전체를 확인하는 게 아니라 최적의 값은 아님\n","\n","<br>\n","\n","- 사용 방법\n","> ```python\n","> from sklearn.model_selection import RandomizedSearchCV\n","> from scipy.stats import uniform\n","> distributions = {'parameter1': ('value1', 'value2', ...), 'parameter2': uniform(loc=1, scale=10)}\n","> grid_search = RandomizedSearchCV(model, parameters, random_state=0)\n","> grid_search.fit(X, y)\n","> ```"]},{"cell_type":"markdown","metadata":{"id":"x0UmmPwdSGhz"},"source":["## Bayesian Optimization\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*PhKGj_bZlND8IEfII426wA.png\" width=\"600\" height=\"400\"/>\n","\n","\n","- 탐색 대상 구간 내의 hyperparameter를 샘플링  \n","- Grid Search나 Random Search의 경우 다음 샘플링 선정 시 이전 샘플링의 정보를 사용하지 못하여 불필요한 탐색을 반복  \n","- Bayesian Optimization은 사전 정보를 활용하여 다음 sample의 후보군을 선택  \n","순차적으로 하이퍼파라미터를 업데이트해가면서 평가를 통해 최적의 하이퍼파라미터 조합 탐색\n","\n","<br>\n","\n","- 사용 방법\n","\n","> ```python\n","> import numpy as np\n","> import lightgbm\n","> from sklearn.metrics import mean_squared_error\n","> from bayes_opt import BayesianOptimization\n","> \n","> def lgbm_cv(\n",">   max_depth,\n",">   learning_rate,\n",">   n_estimators,\n",">   subsample,\n",">   colsample_bytree\n",">   ):\n","> \n",">   # model define\n",">   model = lightgbm.LGBMRegressor(\n",">               max_depth=int(max_depth),\n",">               learning_rate=learning_rate,\n",">               n_estimators=int(n_estimators),\n",">               subsample=subsample,\n",">               colsample_bytree=colsample_bytree,\n",">           )\n",">\n",">   # train\n",">   model.fit(X_train, y_train)\n","> \n",">   # predict\n",">   y_pred = model.predict(X_test)\n","> \n",">   # metric\n",">   rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","> \n",">   # metric return\n",">   return rmse\n","> ```\n","\n","> ```python\n","> pbounds = {\n",">     'max_depth': (3, 10),\n",">     'learning_rate': (0.001, 0.1),\n",">     'n_estimators': (10, 1000),\n",">     'subsample': (0.5, 1),\n",">     'colsample_bytree' :(0.2, 1),\n","> }\n","> \n","> bayesian_optimization = BayesianOptimization(\n",">     f=lgbm_cv,\n",">     pbounds=pbounds,\n",">     verbose=2,          # 출력 옵션\n",">     random_state=0,     # 2: 출력, 1: 최댓값일 때 출력, 0: 출력 안 함\n",">     )\n","> \n","> bayesian_optimization.maximize(init_points=2, n_iter=10, acq='ei', xi=0.01)\n","> # init_points: 초기 random search 수\n","> # n_iter: 반복 횟수\n","> # acq: acquisition function\n","> # xi: exploration 강도\n","> print(bayesian_optimization.max)\n","> ```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNeVwXpYQfQ7lwlMCIkLZ1A","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
