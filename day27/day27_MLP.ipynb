{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UhoL6nMoWa0"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: plotly in c:\\users\\user\\anaconda3\\envs\\deep_learning\\lib\\site-packages (5.24.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\user\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from plotly) (24.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "whwymZ3zVzH2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn                               # layers\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim                         # optimizer\n",
        "import torchvision.datasets as ds                   # mnist daataset\n",
        "import torchvision.transforms as transforms         # image -> tensor\n",
        "from torch.utils.data import DataLoader, Dataset    # torch dataset\n",
        "\n",
        "import matplotlib.pyplot as plt                     # 시각화\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlGXlBEze4XI"
      },
      "source": [
        "# Multi-Layer Pereceptron (MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFUsOXCiVzkP"
      },
      "source": [
        "## MLP란?\n",
        "퍼셉트론으로 이루어진 층(layer) 여러 개를 순차적으로 붙여놓은 형태\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 등장 배경 </font> <p>\n",
        "<font style=\"font-size:16px\"> XOR 게이트 문제 </font> <p>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;and나 or의 경우 직선 하나로 경계를 확정할 수 있으나 xor에서는 불가 <br>\n",
        "\n",
        "  ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FNalwG%2FbtqxdrzsQAV%2FUY3iSEDRakKDjoWEpRHRWk%2Fimg.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 구조 </font> <p>\n",
        "\n",
        "<img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S0925231223004502-gr1.jpg\" width=\"700\" height=\"300\"/>\n",
        "\n",
        "단순 matrix 연산의 반복 <br>\n",
        "> A1 = $\\textbf{W}_1X + \\textbf{b}_1$ <br>\n",
        "> h1 = $\\phi_1$(A1) <br>\n",
        "> A2 = $\\textbf{W}_2\\text{h1} + \\textbf{b}_2 = \\textbf{W}_2\\phi_1(\\textbf{W}_1X + \\textbf{b}_1) + \\textbf{b}_2$ <br>\n",
        "> h2 = $\\phi_2$(A2) = $ \\phi(\\textbf{W}_2\\phi_1(\\textbf{W}_1X + \\textbf{b}_1) + \\textbf{b}_2) $\n",
        "> ...\n",
        "\n",
        "<br>\n",
        "\n",
        "Hidden Layer의 $\\phi$는 activation 함수는 비선형을 목적으로 사용 <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRPTi61Df5IV"
      },
      "source": [
        "## 사용 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPU 사용 확인\n",
        "\n",
        "시스템에서 gpu를 사용할 수 있는지 확인\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seed 고정\n",
        "\n",
        "실험 재현을 위한 난수 고정\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> seed = 0\n",
        "> np.random.seed(seed)\n",
        "> torch.manual_seed(seed)\n",
        "> if device == 'cuda':\n",
        ">     torch.cuda.manual_seed(seed)\n",
        ">     torch.cuda.manual_seed_all(seed)\n",
        ">     torch.backends.cudnn.deterministic = False\n",
        ">     torch.backends.cudnn.benchmark = True\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 하이퍼파라미터 설정\n",
        "\n",
        "하이퍼파라미터: 모델에서 학습되지 않는 사용자가 직접 설정해야하는 파라미터 <br>\n",
        "대표적으로는 아래의 파라미터 존재 <br>\n",
        "- batch_size: 전체 데이터 중 데이터를 묶음으로 분할할 건데 몇 개의 묶음으로 분할할지 설정\n",
        "- epoch: 몇 번의 학습을 진행할 것인가\n",
        "- learning_rate: 얼마 만큼의 보폭으로 학습을 진행할 것인가\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32  # 2의배수를 씀\n",
        "epochs = 100\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_planar_dataset():\n",
        "    np.random.seed(1)\n",
        "    m = 400 # number of examples\n",
        "    N = int(m/2) # number of points per class\n",
        "    D = 2 # dimensionality\n",
        "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
        "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
        "    a = 4 # maximum ray of the flower\n",
        "\n",
        "    for j in range(2):\n",
        "        ix = range(N*j,N*(j+1))\n",
        "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
        "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
        "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "        Y[ix] = j\n",
        "        \n",
        "\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y = load_planar_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(400, 2)"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 결측치 중복 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.DataFrame(X, columns=['X1', 'X2'])\n",
        "data['y'] = y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### train, valid, test split\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/109b64cb7b6e1b6d912b84a1b6afefa53300033be1df4ed8c4433910f8c328bf/68747470733a2f2f76656c6f672e76656c63646e2e636f6d2f696d616765732f696775762f706f73742f38616538343265332d663262362d343463352d623762662d6131663734623361393132342f696d6167652e706e67\" width=\"500\" height=\"300\"/>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> X_train, X_temp, y_train, y_temp = train_test_split(\n",
        ">     X, y, test_size=0.6, random_state=seed, shuffle=True)\n",
        "> X_valid, X_test, y_valid, y_test = train_test_split(\n",
        ">     X, y, test_size=0.5, random_state=seed, shuffle=True)\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=seed, shuffle=True)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=seed, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.971530</td>\n",
              "      <td>1.803161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.929484</td>\n",
              "      <td>-1.390736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.466290</td>\n",
              "      <td>4.037643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.544578</td>\n",
              "      <td>-3.562420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.889885</td>\n",
              "      <td>3.069148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>-1.174729</td>\n",
              "      <td>0.965871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>-0.807037</td>\n",
              "      <td>-3.166023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>-0.952809</td>\n",
              "      <td>-0.056835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>-3.730339</td>\n",
              "      <td>-1.638474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>-0.068807</td>\n",
              "      <td>-1.131119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>240 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1\n",
              "0    2.971530  1.803161\n",
              "1    1.929484 -1.390736\n",
              "2    1.466290  4.037643\n",
              "3    1.544578 -3.562420\n",
              "4   -1.889885  3.069148\n",
              "..        ...       ...\n",
              "235 -1.174729  0.965871\n",
              "236 -0.807037 -3.166023\n",
              "237 -0.952809 -0.056835\n",
              "238 -3.730339 -1.638474\n",
              "239 -0.068807 -1.131119\n",
              "\n",
              "[240 rows x 2 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### scaler 적용\n",
        "\n",
        "일반화 성능을 위해 scaler 적용\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> standard_scaler = StandardScaler()\n",
        "> X_train = standard_scaler.fit_transform(X_train)\n",
        "> X_valid = standard_scaler.fit(X_valid)\n",
        "> X_test = standard_scaler.fit(X_test)\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "standard_scaler_x = StandardScaler()\n",
        "X_train = standard_scaler_x.fit_transform(X_train)\n",
        "X_valid = standard_scaler_x.transform(X_valid)\n",
        "X_test = standard_scaler_x.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.539777</td>\n",
              "      <td>0.865680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.011624</td>\n",
              "      <td>-0.677515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.776858</td>\n",
              "      <td>1.945315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.816537</td>\n",
              "      <td>-1.726808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.924194</td>\n",
              "      <td>1.477367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>-0.561723</td>\n",
              "      <td>0.461127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>-0.375361</td>\n",
              "      <td>-1.535280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>-0.449244</td>\n",
              "      <td>-0.033014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>-1.857014</td>\n",
              "      <td>-0.797214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>-0.001195</td>\n",
              "      <td>-0.552076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>240 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1\n",
              "0    1.539777  0.865680\n",
              "1    1.011624 -0.677515\n",
              "2    0.776858  1.945315\n",
              "3    0.816537 -1.726808\n",
              "4   -0.924194  1.477367\n",
              "..        ...       ...\n",
              "235 -0.561723  0.461127\n",
              "236 -0.375361 -1.535280\n",
              "237 -0.449244 -0.033014\n",
              "238 -1.857014 -0.797214\n",
              "239 -0.001195 -0.552076\n",
              "\n",
              "[240 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset\n",
        "\n",
        "DataLoader에서 불러올 데이터의 형식을 지정하는 클래스. <br>\n",
        "\\_\\_getitem\\_\\_의 idx는 전체 데이터의 인덱스를 임의로 추출. <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;-> def \\_\\_len\\_\\_(self)에서 정의된 데이터의 크기를 사용하기에 **반드시 정의**하여야 함. <br>\n",
        "**Dataset class를 상속**받아 구현\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> class CustomDataset(Dataset):\n",
        ">     def __init__(self, x, y, ...):\n",
        ">         super().__init__()\n",
        ">         self.x = x\n",
        ">         self.y = y\n",
        "> \n",
        ">     def __len__(self):\n",
        ">         return len(self.x)\n",
        "> \n",
        ">     def __getitem__(self, idx):\n",
        ">         x = self.x[idx]\n",
        ">         y = self.y[idx]\n",
        "> \n",
        ">         x = torch.Tensor(x)\n",
        ">         y = torch.Tensor(y)\n",
        ">         \n",
        ">         return x, y\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PlanarDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super().__init__()\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self): # len() 함수를 사용할 수 있게 해줌\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx): # 인덱싱을 가능하게 해줌\n",
        "        X = torch.from_numpy(self.X[idx]).float()\n",
        "        y = torch.from_numpy(self.y[idx]).float()\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = PlanarDataset(X_train, y_train)\n",
        "valid_dataset = PlanarDataset(X_valid, y_valid)\n",
        "test_dataset = PlanarDataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataLoader\n",
        "\n",
        "데이터셋을 순회(iterate)할 때 사용. <br>\n",
        "각 순회(iteration)는 batch_size의 특징(feature)과 정답(label)의 묶음(batch)을 반환. <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> train_dataloader = DataLoader(\n",
        ">     dataset=train_dataset,\n",
        ">     batch_size=batch_size,\n",
        ">     shuffle=False,\n",
        ">     drop_last=True,\n",
        "> )\n",
        "> \n",
        "> valid_dataloader = DataLoader(\n",
        ">     dataset=valid_dataset,\n",
        ">     batch_size=batch_size,\n",
        ">     shuffle=False,\n",
        ">     drop_last=True,\n",
        "> )\n",
        "> \n",
        "> test_dataloader = DataLoader(\n",
        ">     dataset=test_dataset,\n",
        ">     batch_size=batch_size,\n",
        ">     shuffle=False,\n",
        ">     drop_last=True,\n",
        "> )\n",
        "> ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 모델 구조 정의\n",
        "\n",
        "학습 시 사용할 모델 아키텍처 정의. <br>\n",
        "- 모형 복잡도에 비해 데이터 수가 적으면 오버피팅 발생 가능성 높음\n",
        "- 모형 복잡도에 비해 데이터 수가 많으면 언더피팅 발생 가능성 높음 <br>\n",
        "\n",
        "**nn.Module을 상속받아 구현.** <br>\n",
        "생성자에서는 사용할 layer 정의 <br>\n",
        "forward 함수는 순전파 구현 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> class MLP(nn.Module):\n",
        ">     def __init__(self, input_dim, hidden_dim, output_dim):\n",
        ">       super(MLP, self).__init__()\n",
        ">       self.fc1 = nn.Linear(input_dim, hidden_dim)   # input_dim: feature 수, hidden_dim: 중간에 사용될 뉴런의 수\n",
        ">       self.relu = nn.ReLU()                         # ReLU: activation function\n",
        ">       self.fc2 = nn.Linear(hidden_dim, output_dim)  # output_dim: target의 수\n",
        "> \n",
        ">     def forward(self, x):\n",
        ">       x = self.fc1(x)       # input layer -> hidden layer\n",
        ">       x = self.relu(x)      # hidden layer -> activation fuction\n",
        ">       x = self.fc2(x)       # activation function -> output layer\n",
        "> \n",
        ">       return x\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_loader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True, # batch_size로 나누고 남은것들을 버림림\n",
        ")\n",
        "\n",
        "valid_data_loader = DataLoader(\n",
        "    dataset = valid_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False, # 일반적으로 valid와 test는 False를 줌\n",
        "    drop_last = True, # batch_size로 나누고 남은것들을 버림림\n",
        ")\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True, # batch_size로 나누고 남은것들을 버림림\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 모델 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(list(train_data_loader)) # 묶음 수 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y = list(train_data_loader)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 2])"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp = MLP(2, 4, 1).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.5822],\n",
              "        [0.4968],\n",
              "        [0.5637],\n",
              "        [0.6223],\n",
              "        [0.5589],\n",
              "        [0.5530],\n",
              "        [0.5535],\n",
              "        [0.5682],\n",
              "        [0.6236],\n",
              "        [0.4964],\n",
              "        [0.5681],\n",
              "        [0.5931],\n",
              "        [0.5428],\n",
              "        [0.5555],\n",
              "        [0.5049],\n",
              "        [0.6279],\n",
              "        [0.5491],\n",
              "        [0.5745],\n",
              "        [0.5858],\n",
              "        [0.6255],\n",
              "        [0.6353],\n",
              "        [0.6242],\n",
              "        [0.5518],\n",
              "        [0.5903],\n",
              "        [0.5833],\n",
              "        [0.6334],\n",
              "        [0.6322],\n",
              "        [0.5632],\n",
              "        [0.5063],\n",
              "        [0.5513],\n",
              "        [0.5687],\n",
              "        [0.6135]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss, Optimizer 정의\n",
        "\n",
        "학습 시 사용할 loss와 optimizer 명시\n",
        "- loss: 정답과 모델 예측 값의 차이 계산\n",
        "- optimizer: parameter를 update할 로직 설정\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> criterion = nn.CrossEntropyLoss().to(device)\n",
        "> optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss().to(device)\n",
        "optimizer = optim.Adam(mlp.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X13XBcb7sNJ1"
      },
      "source": [
        "### Train & Validation\n",
        "\n",
        "<font style=\"font-size:20px\"> Train </font> <p>\n",
        "정의된 loss와 optimizer를 통해 학습 진행. <br>\n",
        "모델이 주어진 데이터에서만 잘 동작하는 것(overfitting)을 주의해야 함. <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> Validation </font> <p>\n",
        "학습 중인 모델의 성능을 평가하기 위한 데이터. <br>\n",
        "학습 데이터에 포함되어 있지 않기에 학습 시 validation data의 패턴을 학습하지는 않으나 <br>\n",
        "학습되지 않는 데이터를 통해 현재 학습되는 모델의 성능을 평가 <br>\n",
        "오버피팅을 확인하는 목적으로 주로 사용 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "**오버피팅** <br>\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/32695ae0709af770e85679280d75b1d377a43eac73cece31c1b03290ed8cd911/68747470733a2f2f696d67312e6461756d63646e2e6e65742f7468756d622f523132383078302f3f73636f64653d6d746973746f72793226666e616d653d6874747073253341253246253246626c6f672e6b616b616f63646e2e6e6574253246646e2532466f73304535253246627471456d6f5168385a36253246745532756e6636426e464e6d776234597873736f696b253246696d672e706e67\" width=\"500\" height=\"300\"/>\n",
        "\n",
        "주어진 데이터에 모델이 높은 결과를 내도록 적합되는 것 <br>\n",
        "이 경우 일반화 성능이 떨어지기에 오버피팅은 딥러닝 학습에서 중요한 이슈 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> train_losses = []\n",
        "> train_accs = []\n",
        "> valid_losses = []\n",
        "> valid_accs = []\n",
        "> for epoch in tqdm(range(1, epochs+1)):      # 학습 횟수 설정\n",
        ">     total_train_loss = 0\n",
        ">     total_train_acc = 0\n",
        "> \n",
        ">     model.train()                           # dropout, batch norm 등 train과 train이 아닌 경우의 동작이 다를 때 학습 시 적용\n",
        ">     for X, y in train_loader:\n",
        ">         X = X.reshape(batch_size, -1).to(device)\n",
        ">         y = y.to(device)\n",
        "> \n",
        ">         optimizer.zero_grad()               # optimizer의 parameter에 대한 gradient 초기화\n",
        ">         y_pred = model(X)                   # model 예측값 계산 (순전파)\n",
        ">         train_loss = criterion(y_pred, y)   # model 예측값과 실제 값과의 loss 계산\n",
        ">         train_loss.backward()               # 계산된 loss를 바탕으로 각 parameter에 대한 기울기 계산\n",
        ">         optimizer.step()                    # 계산한 기울기를 바탕으로 parameter update\n",
        ">\n",
        ">         total_train_loss += train_loss\n",
        ">         total_train_acc += (y == result.argmax(axis=-1)).sum()\n",
        ">     \n",
        ">     mean_test_loss = total_test_loss / len(test_loader)\n",
        ">     mean_test_acc = total_test_acc / len(test_loader)\n",
        ">     train_losses.append(total_train_loss)\n",
        ">     train_accs.append(total_train_acc)\n",
        "> \n",
        ">     model.eval()                            # dropout, batch norm 등 train과 train이 아닌 경우의 동작이 다를 때 학습이 아닐 시 적용\n",
        ">     with torch.no_grad:                     # 가중치 업데이트를 진행하지 않기에 gradient 제거\n",
        ">         for X, y in valid_loader:\n",
        ">             X = X.reshape(batch_size, -1).to(device)\n",
        ">             y = y.to(device)\n",
        "> \n",
        ">             y_pred = model(X)                \n",
        ">             valid_loss = criterion(y_pred, y)\n",
        "> \n",
        ">             total_valid_loss += valid_loss\n",
        ">             total_valid_acc += (y == result.argmax(axis=-1)).sum()\n",
        "> \n",
        ">         mean_valid_loss = total_valid_loss / len(valid_loader)\n",
        ">         mean_valid_acc = total_valid_acc / len(valid_loader)\n",
        ">         valid_losses.append(mean_valid_loss)\n",
        ">         valid_accs.append(mean_valid_acc)\n",
        ">         \n",
        ">     print(f'Epoch: {epoch: 3d} | train_loss: {mean_train_loss: .6f} | train_acc = {mean_train_acc: .2f}% | valid_loss: {mean_valid_loss: .6f} | valid_acc = {mean_valid_acc: .2f}%')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss:  0.5918 | train_acc:  66.52% | valid_loss:  0.6295 | valid_acc:  54.69%\n",
            "Epoch: 2 | train_loss:  0.5942 | train_acc:  66.96% | valid_loss:  0.6284 | valid_acc:  57.81%\n",
            "Epoch: 3 | train_loss:  0.5968 | train_acc:  66.52% | valid_loss:  0.6278 | valid_acc:  56.25%\n",
            "Epoch: 4 | train_loss:  0.5859 | train_acc:  69.20% | valid_loss:  0.6264 | valid_acc:  56.25%\n",
            "Epoch: 5 | train_loss:  0.5793 | train_acc:  69.20% | valid_loss:  0.6245 | valid_acc:  57.81%\n",
            "Epoch: 6 | train_loss:  0.5820 | train_acc:  67.86% | valid_loss:  0.6221 | valid_acc:  56.25%\n",
            "Epoch: 7 | train_loss:  0.5816 | train_acc:  68.75% | valid_loss:  0.6204 | valid_acc:  54.69%\n",
            "Epoch: 8 | train_loss:  0.5729 | train_acc:  67.86% | valid_loss:  0.6193 | valid_acc:  54.69%\n",
            "Epoch: 9 | train_loss:  0.5615 | train_acc:  69.20% | valid_loss:  0.6171 | valid_acc:  54.69%\n",
            "Epoch: 10 | train_loss:  0.5756 | train_acc:  68.30% | valid_loss:  0.6168 | valid_acc:  54.69%\n",
            "Epoch: 11 | train_loss:  0.5644 | train_acc:  69.20% | valid_loss:  0.6151 | valid_acc:  57.81%\n",
            "Epoch: 12 | train_loss:  0.5800 | train_acc:  68.30% | valid_loss:  0.6117 | valid_acc:  57.81%\n",
            "Epoch: 13 | train_loss:  0.5801 | train_acc:  67.41% | valid_loss:  0.6082 | valid_acc:  59.38%\n",
            "Epoch: 14 | train_loss:  0.5646 | train_acc:  67.86% | valid_loss:  0.6043 | valid_acc:  59.38%\n",
            "Epoch: 15 | train_loss:  0.5680 | train_acc:  70.54% | valid_loss:  0.5992 | valid_acc:  62.50%\n",
            "Epoch: 16 | train_loss:  0.5556 | train_acc:  75.45% | valid_loss:  0.5952 | valid_acc:  64.06%\n",
            "Epoch: 17 | train_loss:  0.5454 | train_acc:  76.34% | valid_loss:  0.5915 | valid_acc:  64.06%\n",
            "Epoch: 18 | train_loss:  0.5593 | train_acc:  75.89% | valid_loss:  0.5853 | valid_acc:  64.06%\n",
            "Epoch: 19 | train_loss:  0.5461 | train_acc:  75.45% | valid_loss:  0.5782 | valid_acc:  67.19%\n",
            "Epoch: 20 | train_loss:  0.5466 | train_acc:  78.12% | valid_loss:  0.5705 | valid_acc:  70.31%\n",
            "Epoch: 21 | train_loss:  0.5318 | train_acc:  79.91% | valid_loss:  0.5614 | valid_acc:  73.44%\n",
            "Epoch: 22 | train_loss:  0.5231 | train_acc:  80.36% | valid_loss:  0.5507 | valid_acc:  76.56%\n",
            "Epoch: 23 | train_loss:  0.5194 | train_acc:  81.70% | valid_loss:  0.5420 | valid_acc:  75.00%\n",
            "Epoch: 24 | train_loss:  0.4985 | train_acc:  84.38% | valid_loss:  0.5324 | valid_acc:  76.56%\n",
            "Epoch: 25 | train_loss:  0.4889 | train_acc:  85.27% | valid_loss:  0.5200 | valid_acc:  79.69%\n",
            "Epoch: 26 | train_loss:  0.4873 | train_acc:  84.38% | valid_loss:  0.5088 | valid_acc:  76.56%\n",
            "Epoch: 27 | train_loss:  0.4670 | train_acc:  86.61% | valid_loss:  0.4994 | valid_acc:  78.12%\n",
            "Epoch: 28 | train_loss:  0.4645 | train_acc:  85.71% | valid_loss:  0.4903 | valid_acc:  79.69%\n",
            "Epoch: 29 | train_loss:  0.4621 | train_acc:  85.71% | valid_loss:  0.4818 | valid_acc:  81.25%\n",
            "Epoch: 30 | train_loss:  0.4530 | train_acc:  86.16% | valid_loss:  0.4727 | valid_acc:  82.81%\n",
            "Epoch: 31 | train_loss:  0.4352 | train_acc:  86.61% | valid_loss:  0.4657 | valid_acc:  81.25%\n",
            "Epoch: 32 | train_loss:  0.4366 | train_acc:  86.61% | valid_loss:  0.4586 | valid_acc:  81.25%\n",
            "Epoch: 33 | train_loss:  0.4261 | train_acc:  86.16% | valid_loss:  0.4524 | valid_acc:  81.25%\n",
            "Epoch: 34 | train_loss:  0.4171 | train_acc:  86.61% | valid_loss:  0.4460 | valid_acc:  81.25%\n",
            "Epoch: 35 | train_loss:  0.4228 | train_acc:  85.27% | valid_loss:  0.4405 | valid_acc:  81.25%\n",
            "Epoch: 36 | train_loss:  0.4091 | train_acc:  86.16% | valid_loss:  0.4352 | valid_acc:  81.25%\n",
            "Epoch: 37 | train_loss:  0.4120 | train_acc:  85.27% | valid_loss:  0.4287 | valid_acc:  82.81%\n",
            "Epoch: 38 | train_loss:  0.3987 | train_acc:  86.16% | valid_loss:  0.4250 | valid_acc:  82.81%\n",
            "Epoch: 39 | train_loss:  0.4062 | train_acc:  86.16% | valid_loss:  0.4186 | valid_acc:  82.81%\n",
            "Epoch: 40 | train_loss:  0.3907 | train_acc:  87.95% | valid_loss:  0.4147 | valid_acc:  82.81%\n",
            "Epoch: 41 | train_loss:  0.3967 | train_acc:  86.16% | valid_loss:  0.4107 | valid_acc:  82.81%\n",
            "Epoch: 42 | train_loss:  0.3945 | train_acc:  86.61% | valid_loss:  0.4051 | valid_acc:  82.81%\n",
            "Epoch: 43 | train_loss:  0.3804 | train_acc:  87.50% | valid_loss:  0.4014 | valid_acc:  82.81%\n",
            "Epoch: 44 | train_loss:  0.3849 | train_acc:  86.61% | valid_loss:  0.3976 | valid_acc:  82.81%\n",
            "Epoch: 45 | train_loss:  0.3831 | train_acc:  86.16% | valid_loss:  0.3942 | valid_acc:  82.81%\n",
            "Epoch: 46 | train_loss:  0.3809 | train_acc:  86.61% | valid_loss:  0.3903 | valid_acc:  84.38%\n",
            "Epoch: 47 | train_loss:  0.3788 | train_acc:  86.61% | valid_loss:  0.3886 | valid_acc:  82.81%\n",
            "Epoch: 48 | train_loss:  0.3552 | train_acc:  89.29% | valid_loss:  0.3855 | valid_acc:  82.81%\n",
            "Epoch: 49 | train_loss:  0.3329 | train_acc:  89.29% | valid_loss:  0.3827 | valid_acc:  82.81%\n",
            "Epoch: 50 | train_loss:  0.3661 | train_acc:  87.95% | valid_loss:  0.3802 | valid_acc:  82.81%\n",
            "Epoch: 51 | train_loss:  0.3559 | train_acc:  87.95% | valid_loss:  0.3764 | valid_acc:  82.81%\n",
            "Epoch: 52 | train_loss:  0.3540 | train_acc:  87.50% | valid_loss:  0.3738 | valid_acc:  84.38%\n",
            "Epoch: 53 | train_loss:  0.3615 | train_acc:  87.50% | valid_loss:  0.3715 | valid_acc:  84.38%\n",
            "Epoch: 54 | train_loss:  0.3707 | train_acc:  87.50% | valid_loss:  0.3694 | valid_acc:  84.38%\n",
            "Epoch: 55 | train_loss:  0.3605 | train_acc:  86.61% | valid_loss:  0.3673 | valid_acc:  84.38%\n",
            "Epoch: 56 | train_loss:  0.3450 | train_acc:  88.84% | valid_loss:  0.3666 | valid_acc:  84.38%\n",
            "Epoch: 57 | train_loss:  0.3554 | train_acc:  88.39% | valid_loss:  0.3632 | valid_acc:  84.38%\n",
            "Epoch: 58 | train_loss:  0.3451 | train_acc:  88.84% | valid_loss:  0.3621 | valid_acc:  84.38%\n",
            "Epoch: 59 | train_loss:  0.3428 | train_acc:  88.39% | valid_loss:  0.3607 | valid_acc:  84.38%\n",
            "Epoch: 60 | train_loss:  0.3309 | train_acc:  88.84% | valid_loss:  0.3584 | valid_acc:  84.38%\n",
            "Epoch: 61 | train_loss:  0.3503 | train_acc:  87.95% | valid_loss:  0.3569 | valid_acc:  84.38%\n",
            "Epoch: 62 | train_loss:  0.3340 | train_acc:  89.29% | valid_loss:  0.3561 | valid_acc:  84.38%\n",
            "Epoch: 63 | train_loss:  0.3178 | train_acc:  89.73% | valid_loss:  0.3558 | valid_acc:  82.81%\n",
            "Epoch: 64 | train_loss:  0.3510 | train_acc:  87.50% | valid_loss:  0.3553 | valid_acc:  82.81%\n",
            "Epoch: 65 | train_loss:  0.3446 | train_acc:  86.61% | valid_loss:  0.3529 | valid_acc:  84.38%\n",
            "Epoch: 66 | train_loss:  0.3396 | train_acc:  88.39% | valid_loss:  0.3494 | valid_acc:  84.38%\n",
            "Epoch: 67 | train_loss:  0.3329 | train_acc:  88.84% | valid_loss:  0.3480 | valid_acc:  84.38%\n",
            "Epoch: 68 | train_loss:  0.3151 | train_acc:  88.84% | valid_loss:  0.3457 | valid_acc:  84.38%\n",
            "Epoch: 69 | train_loss:  0.3255 | train_acc:  89.29% | valid_loss:  0.3440 | valid_acc:  84.38%\n",
            "Epoch: 70 | train_loss:  0.3297 | train_acc:  88.39% | valid_loss:  0.3431 | valid_acc:  84.38%\n",
            "Epoch: 71 | train_loss:  0.3268 | train_acc:  89.29% | valid_loss:  0.3416 | valid_acc:  84.38%\n",
            "Epoch: 72 | train_loss:  0.3281 | train_acc:  88.84% | valid_loss:  0.3409 | valid_acc:  84.38%\n",
            "Epoch: 73 | train_loss:  0.3291 | train_acc:  88.39% | valid_loss:  0.3419 | valid_acc:  84.38%\n",
            "Epoch: 74 | train_loss:  0.3438 | train_acc:  86.61% | valid_loss:  0.3407 | valid_acc:  84.38%\n",
            "Epoch: 75 | train_loss:  0.3324 | train_acc:  87.50% | valid_loss:  0.3378 | valid_acc:  84.38%\n",
            "Epoch: 76 | train_loss:  0.3353 | train_acc:  88.39% | valid_loss:  0.3367 | valid_acc:  84.38%\n",
            "Epoch: 77 | train_loss:  0.3228 | train_acc:  90.62% | valid_loss:  0.3358 | valid_acc:  84.38%\n",
            "Epoch: 78 | train_loss:  0.3258 | train_acc:  89.29% | valid_loss:  0.3360 | valid_acc:  84.38%\n",
            "Epoch: 79 | train_loss:  0.3092 | train_acc:  90.18% | valid_loss:  0.3345 | valid_acc:  84.38%\n",
            "Epoch: 80 | train_loss:  0.3360 | train_acc:  88.84% | valid_loss:  0.3334 | valid_acc:  84.38%\n",
            "Epoch: 81 | train_loss:  0.3290 | train_acc:  89.29% | valid_loss:  0.3323 | valid_acc:  84.38%\n",
            "Epoch: 82 | train_loss:  0.3245 | train_acc:  89.29% | valid_loss:  0.3330 | valid_acc:  84.38%\n",
            "Epoch: 83 | train_loss:  0.3215 | train_acc:  89.29% | valid_loss:  0.3313 | valid_acc:  84.38%\n",
            "Epoch: 84 | train_loss:  0.3329 | train_acc:  87.95% | valid_loss:  0.3320 | valid_acc:  84.38%\n",
            "Epoch: 85 | train_loss:  0.3400 | train_acc:  88.39% | valid_loss:  0.3304 | valid_acc:  84.38%\n",
            "Epoch: 86 | train_loss:  0.3125 | train_acc:  89.73% | valid_loss:  0.3293 | valid_acc:  84.38%\n",
            "Epoch: 87 | train_loss:  0.3181 | train_acc:  89.29% | valid_loss:  0.3288 | valid_acc:  84.38%\n",
            "Epoch: 88 | train_loss:  0.3220 | train_acc:  89.29% | valid_loss:  0.3277 | valid_acc:  84.38%\n",
            "Epoch: 89 | train_loss:  0.3286 | train_acc:  87.95% | valid_loss:  0.3264 | valid_acc:  84.38%\n",
            "Epoch: 90 | train_loss:  0.3265 | train_acc:  88.39% | valid_loss:  0.3256 | valid_acc:  84.38%\n",
            "Epoch: 91 | train_loss:  0.3150 | train_acc:  89.73% | valid_loss:  0.3254 | valid_acc:  84.38%\n",
            "Epoch: 92 | train_loss:  0.3166 | train_acc:  89.29% | valid_loss:  0.3278 | valid_acc:  84.38%\n",
            "Epoch: 93 | train_loss:  0.3115 | train_acc:  90.18% | valid_loss:  0.3265 | valid_acc:  84.38%\n",
            "Epoch: 94 | train_loss:  0.3228 | train_acc:  89.29% | valid_loss:  0.3266 | valid_acc:  84.38%\n",
            "Epoch: 95 | train_loss:  0.3262 | train_acc:  89.29% | valid_loss:  0.3265 | valid_acc:  84.38%\n",
            "Epoch: 96 | train_loss:  0.3144 | train_acc:  90.18% | valid_loss:  0.3245 | valid_acc:  84.38%\n",
            "Epoch: 97 | train_loss:  0.3292 | train_acc:  88.84% | valid_loss:  0.3240 | valid_acc:  84.38%\n",
            "Epoch: 98 | train_loss:  0.3108 | train_acc:  89.29% | valid_loss:  0.3241 | valid_acc:  84.38%\n",
            "Epoch: 99 | train_loss:  0.3197 | train_acc:  88.84% | valid_loss:  0.3229 | valid_acc:  84.38%\n",
            "Epoch: 100 | train_loss:  0.3115 | train_acc:  90.62% | valid_loss:  0.3230 | valid_acc:  84.38%\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "train_accs = []\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    ##################### Train ###########################\n",
        "    total_train_loss = 0\n",
        "    total_train_acc = 0\n",
        "\n",
        "    mlp.train()\n",
        "\n",
        "    for X, y in train_data_loader:\n",
        "        X = X.to(device) # 연산을 위해서 gpu로 보냄\n",
        "        y = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad() # 기울기 초기화\n",
        "        logit = mlp(X)\n",
        "        train_loss = criterion(logit, y) \n",
        "        train_loss.backward()\n",
        "        optimizer.step() # 파라미터 업데이트\n",
        "\n",
        "        predicted_label = torch.where(logit > 0.5, 1, 0)\n",
        "        train_acc = (predicted_label == y).float().mean()\n",
        "        total_train_loss += train_loss\n",
        "        total_train_acc += train_acc\n",
        "\n",
        "    mean_train_loss = total_train_loss /len(train_data_loader)\n",
        "    mean_train_acc = total_train_acc /len(train_data_loader)\n",
        "    train_losses.append(mean_train_loss)\n",
        "    train_accs.append(mean_train_acc)\n",
        "    \n",
        "    ##################### Validation #############################\n",
        "\n",
        "    total_valid_loss = 0\n",
        "    total_valid_acc = 0\n",
        "\n",
        "    mlp.eval()\n",
        "    with torch.no_grad(): # valid는 기울기를 안써서 없애줌줌\n",
        "        for X, y in valid_data_loader:\n",
        "            X = X.to(device) # 연산을 위해서 gpu로 보냄\n",
        "            y = y.to(device)\n",
        "            \n",
        "            logit = mlp(X)\n",
        "            valid_loss = criterion(logit, y) \n",
        "\n",
        "            predicted_label = torch.where(logit > 0.5, 1, 0)\n",
        "            valid_acc = (predicted_label == y).float().mean()\n",
        "            total_valid_loss += valid_loss\n",
        "            total_valid_acc += valid_acc\n",
        "\n",
        "    mean_valid_loss = total_valid_loss /len(valid_data_loader)\n",
        "    mean_valid_acc = total_valid_acc /len(valid_data_loader)\n",
        "    valid_losses.append(mean_valid_loss)\n",
        "    valid_accs.append(mean_valid_acc)\n",
        "\n",
        "    print(f'Epoch: {epoch} | train_loss: {mean_train_loss: .4f} | train_acc: {mean_train_acc*100: .2f}% | valid_loss: {mean_valid_loss: .4f} | valid_acc: {mean_valid_acc*100: .2f}%'  )\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pd.DataFrame({\n",
        "    'train_loss' : [train_loss.item() for train_loss in train_losses],\n",
        "    'valid_loss' : [valid_loss.item() for valid_loss in valid_losses],\n",
        "}).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight',\n",
              "              tensor([[-0.0112,  3.2798],\n",
              "                      [ 2.2422, -2.7789],\n",
              "                      [-0.0090,  3.5127],\n",
              "                      [ 2.5160,  2.9354]])),\n",
              "             ('linear1.bias', tensor([-0.0491, -0.0404, -0.0158,  0.0704])),\n",
              "             ('linear2.weight',\n",
              "              tensor([[ 1.8812,  3.1196,  2.1307, -3.0164]])),\n",
              "             ('linear2.bias', tensor([0.0727]))])"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp.state_dict() # 최종학습 가중치 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(mlp.state_dict(), './model/mlp.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_mlp = MLP(2, 4, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1716\\1103206303.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  new_mlp.load_state_dict(torch.load('./model/mlp.pth'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_mlp.load_state_dict(torch.load('./model/mlp.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight',\n",
              "              tensor([[-0.0112,  3.2798],\n",
              "                      [ 2.2422, -2.7789],\n",
              "                      [-0.0090,  3.5127],\n",
              "                      [ 2.5160,  2.9354]])),\n",
              "             ('linear1.bias', tensor([-0.0491, -0.0404, -0.0158,  0.0704])),\n",
              "             ('linear2.weight',\n",
              "              tensor([[ 1.8812,  3.1196,  2.1307, -3.0164]])),\n",
              "             ('linear2.bias', tensor([0.0727]))])"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_mlp.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "scripted_model = torch.jit.script(mlp)\n",
        "scripted_model.save('./model/scripted_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_scripted_model = torch.jit.load('./model/scripted_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight',\n",
              "              tensor([[-0.0112,  3.2798],\n",
              "                      [ 2.2422, -2.7789],\n",
              "                      [-0.0090,  3.5127],\n",
              "                      [ 2.5160,  2.9354]])),\n",
              "             ('linear1.bias', tensor([-0.0491, -0.0404, -0.0158,  0.0704])),\n",
              "             ('linear2.weight',\n",
              "              tensor([[ 1.8812,  3.1196,  2.1307, -3.0164]])),\n",
              "             ('linear2.bias', tensor([0.0727]))])"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loaded_scripted_model.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_test_loss = 0\n",
        "total_test_acc = 0\n",
        "\n",
        "mlp.eval()\n",
        "with torch.no_grad(): # valid는 기울기를 안써서 없애줌줌\n",
        "    for X, y in test_data_loader:\n",
        "        X = X.to(device) # 연산을 위해서 gpu로 보냄\n",
        "        y = y.to(device)\n",
        "        \n",
        "        logit = mlp(X)\n",
        "        test_loss = criterion(logit, y) \n",
        "\n",
        "        predicted_label = torch.where(logit > 0.5, 1, 0)\n",
        "        test_acc = (predicted_label == y).float().mean()\n",
        "        total_test_loss += test_loss\n",
        "        total_test_acc += test_acc\n",
        "\n",
        "    mean_test_loss = total_test_loss /len(test_data_loader)\n",
        "    mean_test_acc = total_test_acc /len(test_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.4125)"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.859375"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_test_acc.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test\n",
        "\n",
        "학습에서 사용한 모델의 성능 평가. <br>\n",
        "train의 정확도가 높았을지라도 test의 정확도가 낮을 수 있음. <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;이는 학습 데이터와 test 데이터가 상이함을 의미. <br>\n",
        "학습 평가 시 eval() 사용. <br>\n",
        "validation data와의 차이는 validation data는 학습 중에 사용되는 데이터이지만 <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;테스트 데이터는 학습이 종료된 이후 사용되는 데이터 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 사용 예제 </font> <p>\n",
        "\n",
        "> ```python\n",
        "> model.eval()                            # dropout, batch norm 등 train과 train이 아닌 경우의 동작이 다를 때 학습이 아닐 시 적용\n",
        "> with torch.no_grad:                     # 가중치 업데이트를 진행하지 않기에 gradient 제거\n",
        ">     for X, y in test_loader:\n",
        ">         X = X.reshape(batch_size, -1).to(device)\n",
        ">         y = y.to(device)\n",
        "> \n",
        ">         y_pred = model(X)                \n",
        ">         test_loss = criterion(y_pred, y)\n",
        "> \n",
        ">         total_test_loss += test_loss\n",
        ">         total_test_acc += (y == result.argmax(axis=-1)).sum()\n",
        "> \n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save & Load\n",
        "\n",
        "학습된 모델의 weight를 저장 및 로드\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> Save </font> <p>\n",
        "\n",
        "> ```python\n",
        "> torch.save(model.state_dict(), './checkpoint/model_state_dict.pth')\n",
        "> ```\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> Load </font> <p>\n",
        "\n",
        "> ```python\n",
        "> model = Model()\n",
        "> model.load_state_dict(torch.load('./checkpoint/model_state_dict.pth'), map_location='cpu')\n",
        "> ```\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> torch script </font> <p>\n",
        "\n",
        "```python\n",
        "scripted_model = torch.jit.script(model)\n",
        "\n",
        "# save\n",
        "scripted_model.save('./checkpoint/scripted_model.pt')\n",
        "\n",
        "# load\n",
        "loaded_model = torch.jit.load('./checkpoint/scripted_model.pt')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEim4kEdi4VB"
      },
      "source": [
        "## MNIST\n",
        "\n",
        "- MNIST 데이터베이스 (Modified National Institute of Standards and Technology database)는 <br>\n",
        "손으로 쓴 숫자들로 이루어진 대형 데이터베이스\n",
        "- 딥러닝 예제에서 자주 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 1/20 [00:00<00:15,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss:  1.6072 | train_acc:  87.02% | valid_loss:  1.5341 | valid_acc:  93.06%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 2/20 [00:01<00:13,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | train_loss:  1.5217 | train_acc:  94.35% | valid_loss:  1.5121 | valid_acc:  95.31%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 3/20 [00:02<00:12,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 | train_loss:  1.5054 | train_acc:  95.88% | valid_loss:  1.5036 | valid_acc:  95.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 4/20 [00:02<00:11,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 | train_loss:  1.4974 | train_acc:  96.58% | valid_loss:  1.4972 | valid_acc:  96.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 5/20 [00:03<00:10,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 | train_loss:  1.4908 | train_acc:  97.24% | valid_loss:  1.4993 | valid_acc:  96.41%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 6/20 [00:04<00:10,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6 | train_loss:  1.4867 | train_acc:  97.61% | valid_loss:  1.4921 | valid_acc:  97.09%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 7/20 [00:05<00:09,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7 | train_loss:  1.4837 | train_acc:  97.89% | valid_loss:  1.4958 | valid_acc:  96.64%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 8/20 [00:05<00:08,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8 | train_loss:  1.4811 | train_acc:  98.13% | valid_loss:  1.4893 | valid_acc:  97.37%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 9/20 [00:06<00:08,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9 | train_loss:  1.4784 | train_acc:  98.38% | valid_loss:  1.4887 | valid_acc:  97.36%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 10/20 [00:07<00:07,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | train_loss:  1.4781 | train_acc:  98.39% | valid_loss:  1.4877 | valid_acc:  97.49%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 11/20 [00:08<00:06,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11 | train_loss:  1.4757 | train_acc:  98.64% | valid_loss:  1.4868 | valid_acc:  97.53%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 12/20 [00:09<00:06,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12 | train_loss:  1.4757 | train_acc:  98.62% | valid_loss:  1.4867 | valid_acc:  97.54%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 13/20 [00:09<00:05,  1.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13 | train_loss:  1.4740 | train_acc:  98.80% | valid_loss:  1.4889 | valid_acc:  97.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 14/20 [00:10<00:04,  1.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14 | train_loss:  1.4738 | train_acc:  98.80% | valid_loss:  1.4883 | valid_acc:  97.35%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 15/20 [00:11<00:04,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15 | train_loss:  1.4734 | train_acc:  98.84% | valid_loss:  1.4880 | valid_acc:  97.31%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 16/20 [00:12<00:03,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16 | train_loss:  1.4721 | train_acc:  98.94% | valid_loss:  1.4856 | valid_acc:  97.62%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 17/20 [00:13<00:02,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17 | train_loss:  1.4712 | train_acc:  99.06% | valid_loss:  1.4847 | valid_acc:  97.69%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 18/20 [00:14<00:01,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18 | train_loss:  1.4715 | train_acc:  99.00% | valid_loss:  1.4873 | valid_acc:  97.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 19/20 [00:15<00:00,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19 | train_loss:  1.4704 | train_acc:  99.11% | valid_loss:  1.4842 | valid_acc:  97.72%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:16<00:00,  1.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20 | train_loss:  1.4698 | train_acc:  99.18% | valid_loss:  1.4859 | valid_acc:  97.48%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# seed\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# hyperparameter\n",
        "batch_size = 256\n",
        "epochs = 20\n",
        "learning_rate = 4e-3\n",
        "\n",
        "# data and parameter\n",
        "\n",
        "train = ds.MNIST(\n",
        "    root='data/mnist',\n",
        "    train=True,\n",
        "    transform = transforms.ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "\n",
        "test = ds.MNIST(\n",
        "    root='data/mnist',\n",
        "    train=False,\n",
        "    transform = transforms.ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "\n",
        "train, valid = train_test_split(train, test_size=0.2)\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        X, y = self.data[idx]\n",
        "\n",
        "        return X, y\n",
        "    \n",
        "\n",
        "\n",
        "train_dataset = MNISTDataset(train)\n",
        "valid_dataset = MNISTDataset(valid)\n",
        "test_dataset = MNISTDataset(test)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True,\n",
        ")\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    dataset = valid_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = True,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = True,\n",
        ")\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "\n",
        "model = Model(28*28, 256, 10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "for epoch in tqdm(range(1,epochs+1)):\n",
        "    total_train_acc =0\n",
        "    total_train_loss =0\n",
        "\n",
        "    model.train()\n",
        "    for X, y in train_dataloader:\n",
        "        X = X.to(device)\n",
        "        X = X.flatten(start_dim =1)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "        logit = F.softmax(output, dim =-1)\n",
        "        train_loss = criterion(logit, y)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc = (logit.argmax(dim=-1) == y).float().mean()\n",
        "        total_train_acc += train_acc\n",
        "        total_train_loss += train_loss\n",
        "\n",
        "    mean_train_acc = total_train_acc / len(train_dataloader)\n",
        "    mean_train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_losses.append(mean_train_loss.item())\n",
        "    train_accs.append(mean_train_acc)\n",
        "\n",
        "    total_valid_loss = 0\n",
        "    total_valid_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in valid_dataloader:\n",
        "            X = X.to(device)\n",
        "            X = X.flatten(start_dim = 1)\n",
        "            y = y.to(device)\n",
        "\n",
        "            \n",
        "            output = model(X)\n",
        "            logit = F.softmax(output, dim =-1)\n",
        "            valid_loss = criterion(logit, y)\n",
        "            \n",
        "\n",
        "            valid_acc = (logit.argmax(dim=-1) == y).float().mean()\n",
        "            total_valid_acc += valid_acc\n",
        "            total_valid_loss += valid_loss\n",
        "\n",
        "    mean_valid_acc = total_valid_acc / len(valid_dataloader)\n",
        "    mean_valid_loss = total_valid_loss / len(valid_dataloader)\n",
        "    valid_losses.append(mean_valid_loss.item())\n",
        "    valid_accs.append(mean_valid_acc)\n",
        "    \n",
        "    print(f'Epoch: {epoch} | train_loss: {mean_train_loss: .4f} | train_acc: {mean_train_acc*100: .2f}% | valid_loss: {mean_valid_loss: .4f} | valid_acc: {mean_valid_acc*100: .2f}%'  )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09815704822540283\n"
          ]
        }
      ],
      "source": [
        "total_test_acc = 0\n",
        "total_test_loss = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        X = X.flatten(start_dim=1)\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = model(X)\n",
        "        logit = F.softmax(output, dim=-1)\n",
        "        test_loss = criterion(logit, y)\n",
        "\n",
        "        test_acc = (logit.argmax(dim=-1) == y).float().mean()\n",
        "        total_test_acc += test_acc\n",
        "        total_test_loss += test_loss\n",
        "    \n",
        "mean_test_acc = total_test_acc / len(test_dataloader)\n",
        "print(mean_test_acc.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([256, 1, 28, 28])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(train_dataloader)[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.nn.modules.module.Module"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (7168x28 and 784x256)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[43], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m logit \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(output, dim \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(logit, y)\n",
            "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[27], line 14\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
            "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (7168x28 and 784x256)"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([7, 3, 2, 3, 7, 2, 0, 7, 1, 9, 7, 9, 1, 7, 0, 2, 2, 7, 9, 6, 8, 1, 5, 9,\n",
              "        3, 2, 2, 7, 8, 4, 0, 5, 2, 2, 4, 7, 5, 1, 0, 4, 8, 9, 1, 0, 3, 2, 3, 1,\n",
              "        1, 0, 6, 5, 0, 2, 9, 9, 5, 8, 0, 1, 8, 1, 9, 4, 8, 3, 4, 4, 4, 1, 0, 0,\n",
              "        7, 3, 2, 2, 2, 8, 1, 7, 4, 0, 6, 1, 2, 4, 8, 0, 4, 3, 2, 6, 7, 1, 3, 4,\n",
              "        9, 3, 0, 7, 5, 4, 6, 8, 4, 4, 5, 5, 7, 4, 1, 5, 3, 3, 2, 8, 4, 6, 1, 0,\n",
              "        4, 4, 3, 5, 9, 9, 0, 3, 3, 8, 0, 6, 8, 7, 9, 0, 0, 0, 1, 1, 2, 3, 5, 3,\n",
              "        1, 3, 7, 1, 7, 7, 2, 8, 8, 6, 7, 2, 5, 1, 5, 5, 7, 8, 1, 2, 5, 5, 0, 5,\n",
              "        8, 0, 4, 1, 3, 9, 3, 7, 9, 7, 4, 4, 8, 0, 6, 3, 5, 6, 2, 2, 7, 9, 3, 3,\n",
              "        0, 2, 1, 0, 1, 8, 2, 0, 5, 3, 0, 1, 4, 9, 8, 3, 6, 3, 5, 9, 8, 2, 9, 1,\n",
              "        6, 5, 0, 4, 4, 5, 5, 6, 1, 0, 1, 8, 7, 7, 2, 7, 1, 1, 4, 0, 5, 9, 3, 1,\n",
              "        1, 5, 5, 6, 4, 4, 1, 1, 3, 3, 3, 3, 3, 0, 8, 4])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_valid_loss = 0\n",
        "total_valid_acc = 0\n",
        "\n",
        "mlp.eval()\n",
        "with torch.no_grad(): # valid는 기울기를 안써서 없애줌줌\n",
        "    for X, y in valid_data_loader:\n",
        "        X = X.to(device) # 연산을 위해서 gpu로 보냄\n",
        "        y = y.to(device)\n",
        "        \n",
        "        logit = mlp(X)\n",
        "        valid_loss = criterion(logit, y) \n",
        "\n",
        "        predicted_label = torch.where(logit > 0.5, 1, 0)\n",
        "        valid_acc = (predicted_label == y).float().mean()\n",
        "        total_valid_loss += valid_loss\n",
        "        total_valid_acc += valid_acc\n",
        "\n",
        "mean_valid_loss = total_valid_loss /len(valid_data_loader)\n",
        "mean_valid_acc = total_valid_acc /len(valid_data_loader)\n",
        "valid_losses.append(mean_valid_loss)\n",
        "valid_accs.append(mean_valid_acc)\n",
        "\n",
        "print(f'Epoch: {epoch} | train_loss: {mean_train_loss: .4f} | train_acc: {mean_train_acc*100: .2f}% | valid_loss: {mean_valid_loss: .4f} | valid_acc: {mean_valid_acc*100: .2f}%'  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diamonds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>carat</th>\n",
              "      <th>cut</th>\n",
              "      <th>color</th>\n",
              "      <th>clarity</th>\n",
              "      <th>depth</th>\n",
              "      <th>table</th>\n",
              "      <th>price</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.23</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>E</td>\n",
              "      <td>SI2</td>\n",
              "      <td>61.5</td>\n",
              "      <td>55.0</td>\n",
              "      <td>326</td>\n",
              "      <td>3.95</td>\n",
              "      <td>3.98</td>\n",
              "      <td>2.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>Premium</td>\n",
              "      <td>E</td>\n",
              "      <td>SI1</td>\n",
              "      <td>59.8</td>\n",
              "      <td>61.0</td>\n",
              "      <td>326</td>\n",
              "      <td>3.89</td>\n",
              "      <td>3.84</td>\n",
              "      <td>2.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.23</td>\n",
              "      <td>Good</td>\n",
              "      <td>E</td>\n",
              "      <td>VS1</td>\n",
              "      <td>56.9</td>\n",
              "      <td>65.0</td>\n",
              "      <td>327</td>\n",
              "      <td>4.05</td>\n",
              "      <td>4.07</td>\n",
              "      <td>2.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.29</td>\n",
              "      <td>Premium</td>\n",
              "      <td>I</td>\n",
              "      <td>VS2</td>\n",
              "      <td>62.4</td>\n",
              "      <td>58.0</td>\n",
              "      <td>334</td>\n",
              "      <td>4.20</td>\n",
              "      <td>4.23</td>\n",
              "      <td>2.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.31</td>\n",
              "      <td>Good</td>\n",
              "      <td>J</td>\n",
              "      <td>SI2</td>\n",
              "      <td>63.3</td>\n",
              "      <td>58.0</td>\n",
              "      <td>335</td>\n",
              "      <td>4.34</td>\n",
              "      <td>4.35</td>\n",
              "      <td>2.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53935</th>\n",
              "      <td>0.72</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>D</td>\n",
              "      <td>SI1</td>\n",
              "      <td>60.8</td>\n",
              "      <td>57.0</td>\n",
              "      <td>2757</td>\n",
              "      <td>5.75</td>\n",
              "      <td>5.76</td>\n",
              "      <td>3.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53936</th>\n",
              "      <td>0.72</td>\n",
              "      <td>Good</td>\n",
              "      <td>D</td>\n",
              "      <td>SI1</td>\n",
              "      <td>63.1</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2757</td>\n",
              "      <td>5.69</td>\n",
              "      <td>5.75</td>\n",
              "      <td>3.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53937</th>\n",
              "      <td>0.70</td>\n",
              "      <td>Very Good</td>\n",
              "      <td>D</td>\n",
              "      <td>SI1</td>\n",
              "      <td>62.8</td>\n",
              "      <td>60.0</td>\n",
              "      <td>2757</td>\n",
              "      <td>5.66</td>\n",
              "      <td>5.68</td>\n",
              "      <td>3.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53938</th>\n",
              "      <td>0.86</td>\n",
              "      <td>Premium</td>\n",
              "      <td>H</td>\n",
              "      <td>SI2</td>\n",
              "      <td>61.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>2757</td>\n",
              "      <td>6.15</td>\n",
              "      <td>6.12</td>\n",
              "      <td>3.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53939</th>\n",
              "      <td>0.75</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>D</td>\n",
              "      <td>SI2</td>\n",
              "      <td>62.2</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2757</td>\n",
              "      <td>5.83</td>\n",
              "      <td>5.87</td>\n",
              "      <td>3.64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>53940 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       carat        cut color clarity  depth  table  price     x     y     z\n",
              "0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
              "1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
              "2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
              "3       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
              "4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
              "...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n",
              "53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50\n",
              "53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61\n",
              "53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56\n",
              "53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74\n",
              "53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64\n",
              "\n",
              "[53940 rows x 10 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diamonds = sns.load_dataset('diamonds')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "diamonds"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
